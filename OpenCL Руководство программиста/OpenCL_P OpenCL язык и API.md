
OpenCL язык и API
1. [[#Введение в OpenCL]] 1.1
2. [[#Что такое OpenCL, или ... зачем вам нужна эта книга]] 1.2
3. [[#Наше многоядерное будущее гетерогенные платформы.|Наше многоядерное будущее: гетерогенные платформы.]] 1.3
4. [[#Программное обеспечение в многоядерном мире.|Программное обеспечение в многоядерном мире.]] 1.4
5. [[#Концептуальные основы OpenCL|Концептуальные основы OpenCL]] 1.5
6. [[#Модель платформы|Модель платформы]] 1.6
7. [[#Модель выполнения|Модель выполнения]] 1.7
8. [[#Контекст|Контекст]] 1.8
9. [[#Очереди команд|Очереди команд]] 1.9
10. [[#Модель памяти|Модель памяти]] 1.10

# Введение в OpenCL

При изучении новой модели программирования легко потеряться в море деталей. API и странная новая терминология, казалось бы, появляются из ниоткуда, создавая ненужную сложность и запутывая. Главное — начать с чёткого понимания на высоком уровне, чтобы иметь представление, к которому можно обратиться, когда станет трудно. Цель этой главы — помочь вам составить такое представление. Мы начнём с краткого обзора спецификации OpenCL 1.1 и тенденций в области гетерогенных вычислений, которые делают её таким важным стандартом программирования. Затем мы опишем концептуальные модели, лежащие в основе OpenCL, и используем их для объяснения принципов работы OpenCL. На этом этапе мы закладываем теоретическую основу OpenCL и переходим к рассмотрению компонентов OpenCL. Ключевым моментом является то, как OpenCL работает с графическими стандартами. Мы завершим обзор OpenCL кратким рассмотрением того, как стандарт OpenCL работает со встроенными процессорами.

# Что такое OpenCL, или ... зачем вам нужна эта книга

OpenCL — это отраслевой стандарт для программирования компьютеров, состоящих из комбинации центральных процессоров, графических процессоров и других процессоров. Эти так называемые гетерогенные системы стали важным классом платформ, и OpenCL — первый отраслевой стандарт, напрямую отвечающий их потребностям. OpenCL — относительно новая технология, впервые представленная в декабре 2008 года, а первые продукты появились осенью 2009 года. С помощью OpenCL можно написать одну программу, которая будет работать на самых разных системах: от мобильных телефонов до ноутбуков и узлов в огромных суперкомпьютерах. Ни один другой стандарт параллельного программирования не имеет такого широкого охвата. Это одна из причин, почему OpenCL так важен и может изменить индустрию программного обеспечения. Это также источник большей части критики в адрес OpenCL.

OpenCL обеспечивает высокий уровень переносимости, предоставляя доступ к аппаратному обеспечению, а не скрывая его за элегантными абстракциями. Это означает, что программист OpenCL должен явно определять платформу, её контекст и то, как работа распределяется между различными устройствами. Не всем программистам нужен или даже желателен детальный контроль, который обеспечивает OpenCL. И это нормально; если он доступен, то высокоуровневая модель программирования часто является лучшим подходом. Однако даже высокоуровневым моделям программирования нужна прочная (и переносимая) основа, и OpenCL может стать такой основой. Эта книга представляет собой подробное введение в OpenCL. Хотя любой может скачать спецификацию (www.khronos.org/opencl) и выучить все конструкции OpenCL, в спецификации не описано, как использовать OpenCL для решения задач. В этом и заключается смысл этой книги: решение задач с помощью фреймворка OpenCL.

# Наше многоядерное будущее: гетерогенные платформы.

За последнее десятилетие компьютеры претерпели фундаментальные изменения. Раньше инновации зависели от чистой производительности. Однако несколько лет назад акцент сместился на производительность, приходящуюся на один ватт потребляемой энергии. Полупроводниковые компании будут продолжать размещать всё больше и больше транзисторов на одном кристалле, но эти производители будут конкурировать за энергоэффективность, а не за чистую производительность. Этот сдвиг радикально изменил компьютеры, которые производит отрасль. Во-первых, микропроцессоры внутри наших компьютеров состоят из нескольких маломощных ядер. Многоядерный императив был впервые изложен А. П. Чан-дракасаном и др. в статье “Оптимизация мощности с использованием преобразований”.1 Суть их аргументации можно найти на рисунке 1.1. Энергия, затрачиваемая на переключение вентилей в процессоре, равна емкости (C), умноженной на квадрат напряжения (V). Эти вентили переключаются в течение секунды количество раз, равное частоте. Следовательно, мощность микропроцессора измеряется как P = CV2f. Если мы сравним одноядерный процессор, работающий на частоте f и напряжении V, с аналогичным процессором с двумя ядрами, каждое из которых работает на частоте f/2, то мы увеличим количество схем в чипе. Согласно моделям, описанным в разделе «Оптимизация энергопотребления с помощью преобразований», номинально это увеличивает ёмкость в 2,2 раза. Но напряжение существенно падает до 0,6 В. Таким образом, количество выполненных инструкций  в секунду одинаково, но энергопотребление двухъядерного варианта составляет 0,396 от энергопотребления одноядерного. Именно эта фундаментальная взаимосвязь лежит в основе перехода к многоядерным чипам. Множество ядер, работающих на более низких частотах, принципиально более энергоэффективны.
![[OpenCL_P_1.png]]

рис. 1.1. В этих двух случаях скорость выполнения инструкций одинакова, но мощность намного меньше при использовании двух ядер, работающих на половинной частоте одного ядра.

Следующий вопрос: «Будут ли эти ядра одинаковыми (однородными) или они будут отличаться друг от друга?» Чтобы понять эту тенденцию, рассмотрим энергоэффективность специализированной логики по сравнению с логикой общего назначения. Процессор общего назначения по своей природе должен включать в себя широкий спектр функциональных блоков, чтобы удовлетворять любые вычислительные потребности. Именно это делает чип процессором общего назначения. Однако в процессорах, специализированных для выполнения определённой функции, меньше лишних транзисторов, поскольку они включают только те функциональные блоки, которые требуются для выполнения этой функции. Результат можно увидеть на рисунке 1.2, где мы сравниваем процессор общего назначения (Intel Core 2 Quad, модель Q6700)2, графический процессор (NVIDIA GTX 280) и узкоспециализированный исследовательский процессор (Intel 80-ядерный исследовательский процессор Tera-scale, ядра которого представляют собой просто пару арифметических блоков умножения-суммирования с плавающей запятой). Чтобы сравнение было максимально объективным, каждый из чипов был изготовлен по техпроцессу 65 нм, и мы использовали опубликованную производителем максимальную производительность в зависимости от расчётной тепловой мощности. Как видно на рисунке, чем более специализирован процессор, тем выше его энергоэффективность, если задачи хорошо подходят для него.

![[OpenCL_P_2.png]]
рис. 1.2. График зависимости пиковой производительности от мощности в точке теплового проектирования для трёх процессоров, произведённых по 65-нм техпроцессу. Примечание: это не значит, что один процессор лучше или хуже других. Дело в том, что чем более специализированно ядро, тем оно энергоэффективнее.

Таким образом, есть все основания полагать, что в мире, где важна максимальная производительность на ватт, мы можем ожидать, что системы будут всё больше зависеть от множества ядер со специализированным кремнием, где это возможно. Это особенно важно для мобильных устройств, в которых критически важно экономить заряд батареи. Однако это неоднородное будущее уже наступило. Рассмотрим схематическое изображение современного ПК на рисунке 1.3. Здесь есть два разъёма, в каждом из которых потенциально может находиться многоядерный процессор; контроллер графики и памяти (GMCH), который подключается к системной памяти (DRAM); и графический процессор (GPU). Это гетерогенная платформа с несколькими наборами команд и несколькими уровнями параллелизма, которые необходимо использовать, чтобы раскрыть весь потенциал системы.

![[OpenCL_P_3.png]]
рис. 1.3. Блок-схема современного настольного ПК с несколькими процессорами (потенциально разными) и графическим процессором, демонстрирующая, что современные системы часто бывают неоднородными

Базовая платформа как сегодня, так и в будущем, на высоком уровне ясна. Множество деталей и инноваций, несомненно, удивят нас, но тенденции в области аппаратного обеспечения очевидны. Будущее принадлежит гетерогенным многоядерным платформам. Вопрос, который стоит перед нами, заключается в том, как наше программное обеспечение должно адаптироваться к этим платформам.

# Программное обеспечение в многоядерном мире.

Параллельное аппаратное обеспечение обеспечивает производительность за счёт одновременного выполнения нескольких операций. Чтобы быть полезным, параллельное аппаратное обеспечение нуждается в программном обеспечении, которое выполняет несколько потоков операций одновременно; другими словами, вам нужно параллельное программное обеспечение. 

Чтобы понять, что такое параллельное программное обеспечение, мы должны начать с более общей концепции параллелизма. ***Параллелизм*** — это старая и хорошо знакомая концепция в информатике. Программная система является параллельной, если она состоит из нескольких потоков операций, которые активны и могут выполняться одновременно. Параллелизм является основополагающим принципом любой современной операционной системы. Он позволяет максимально эффективно использовать ресурсы, позволяя другим потокам операций (нитям) выполняться, пока другие приостановлены в ожидании какого-либо ресурса. Он создаёт у пользователя, взаимодействующего с системой, иллюзию непрерывного и почти мгновенного взаимодействия с системой.

Когда параллельное программное обеспечение работает на компьютере с несколькими процессорами, так что потоки фактически выполняются одновременно, мы имеем дело с параллельными вычислениями. Параллелизм, обеспечиваемый аппаратным обеспечением, — это одновременная работа нескольких процессов. 

Задача программистов — найти параллелизм в своей задаче, выразить его в программном обеспечении, а затем запустить полученную программу так, чтобы параллелизм обеспечивал желаемую производительность. Найти параллелизм в задаче может быть так же просто, как выполнить независимый поток операций для каждого пикселя изображения. Или это может быть невероятно сложно из-за множества потоков операций, которые обмениваются информацией и должны тесно взаимодействовать при выполнении.

Как только в задаче обнаруживается параллелизм, программисты должны выразить этот параллелизм в исходном коде. В частности, необходимо определить потоки операций, которые будут выполняться параллельно, связать с ними данные, с которыми они работают, и управлять зависимостями между ними, чтобы при их параллельном выполнении получался правильный ответ. В этом и заключается суть проблемы параллельного программирования. 

Манипулирование низкоуровневыми деталями параллельного компьютера находится за пределами возможностей большинства людей. Даже опытные программисты, работающие с параллельными вычислениями, были бы перегружены необходимостью управлять каждым конфликтом памяти или планировать отдельные потоки. Следовательно, ключом к параллельному программированию является высокоуровневая абстракция или модель, которая делает задачу параллельного программирования более управляемой.

Существует слишком много моделей программирования, разделённых на пересекающиеся категории с запутанными и часто неоднозначными названиями. Для наших целей мы рассмотрим две модели параллельного программирования: параллелизм задач и параллелизм данных. На высоком уровне идеи, лежащие в основе этих двух моделей, просты.

В модели программирования с параллельной обработкой данных программисты рассматривают свои задачи как наборы элементов данных, которые можно обновлять одновременно. Параллелизм выражается в одновременном применении одного и того же потока инструкций (задачи) к каждому элементу данных. Параллелизм заключается в данных. На рисунке 1.4 приведён простой пример параллельной обработки данных. Рассмотрим простую задачу, которая просто возвращает квадрат входного значения и вектор чисел (`A_vector`). Используя модель программирования с параллельной обработкой данных, мы обновляем вектор параллельно, определяя, что задача должна быть применена к каждому элементу для получения нового результирующего вектора. Конечно, этот пример очень простой. На практике количество операций в задаче должно быть большим, чтобы компенсировать затраты на перемещение данных и управлять параллельными вычислениями. Но простой пример на рисунке отражает ключевую идею этого режима программирования.

![[OpenCL_P_4.png]]
Рис. 1.4. Простой пример параллельной обработки данных, когда к каждому элементу вектора одновременно применяется одна задача для создания нового вектора

В модели программирования с разделением задач на параллельные потоки программисты напрямую определяют и управляют параллельными задачами. Проблемы разбиваются на задачи, которые могут выполняться параллельно, а затем распределяются по вычислительным элементам (PE) параллельного компьютера для выполнения. Это проще всего сделать, когда задачи полностью независимы, но эта модель программирования также используется для задач, которые используют общие данные. Вычисления с набором задач завершаются, когда выполняется последняя задача. Поскольку задачи сильно различаются по вычислительным требованиям, их распределение таким образом, чтобы все они выполнялись примерно в одно и то же время, может быть затруднительным. Это проблема балансировки нагрузки. Рассмотрим пример на рисунке 1.5, где у нас есть шесть независимых задач, которые нужно выполнить одновременно на трёх PE (процессорный элемент). В одном случае у первого процессорного элемента есть дополнительная работа, и он работает значительно дольше, чем другие процессорные элементы. Во втором случае с другим распределением задач показан более идеальный вариант, при котором каждый процессорный элемент завершает работу примерно в одно и то же время. Это пример ключевого идеала параллельных вычислений, называемого ***балансировкой нагрузки***.

Выбор между параллелизмом данных и параллелизмом задач зависит от потребностей решаемой задачи. Например, задачи, связанные с обновлением точек на сетке, сразу же можно решить с помощью моделей параллелизма данных. С другой стороны, задачи, связанные с обходом графов, естественным образом решаются с помощью параллелизма задач. Следовательно, программисту, работающему с параллельными вычислениями, необходимо разбираться в обеих моделях программирования. А общая платформа программирования (например, OpenCL) должна поддерживать обе модели.

Независимо от модели программирования, следующим шагом в процессе параллельного программирования является привязка программы к реальному оборудованию. Именно на этом этапе возникают уникальные проблемы, связанные с разнородными компьютерами. Вычислительные элементы в системе могут иметь разные наборы команд и архитектуры памяти и работать с разной скоростью. Эффективная программа должна учитывать эти различия и соответствующим образом привязывать параллельное программное обеспечение к наиболее подходящим устройствам OpenCL.

![[OpenCL_P_5.png]]
рис. 1.5. Параллелизм задач, демонстрирующий два способа распределения шести независимых задач между тремя исполнительными элементами. Вычисления не выполняются до тех пор, пока не будет завершена каждая задача, поэтому целью должна быть сбалансированная нагрузка, то есть одинаковое время выполнения вычислений каждым исполнительным элементом.

Традиционно программисты решали эту проблему, рассматривая своё программное обеспечение как набор модулей, реализующих отдельные части задачи. Модули напрямую связаны с компонентами гетерогенной платформы. Например, графическое программное обеспечение работает на графическом процессоре. Другое программное обеспечение работает на центральном процессоре.

Программирование графических процессоров общего назначения (GPGPU) нарушило эту модель. Алгоритмы, не связанные с графикой, были модифицированы для работы на графическом процессоре. Центральный процессор настраивает вычисления и управляет вводом-выводом, но все «интересные» вычисления переносятся на графический процессор. По сути, гетерогенная платформа игнорируется, а основное внимание уделяется одному компоненту системы — графическому процессору.

OpenCL не рекомендует использовать такой подход. По сути, пользователь «платит за все устройства OpenCL» в системе, поэтому эффективная программа должна использовать их все. Именно это OpenCL рекомендует делать программистам и чего вы ожидаете от среды программирования, разработанной для гетерогенных платформ.

Аппаратная неоднородность сложна. Программисты привыкли полагаться на высокоуровневые абстракции, которые скрывают сложность аппаратного обеспечения. Неоднородный язык программирования выявляет неоднородность и противоречит тенденции к повышению уровня абстракции.

# Концептуальные основы OpenCL

Как мы увидим далее в этой книге, OpenCL поддерживает широкий спектр приложений. Трудно сделать какие-либо обобщения об этих приложениях. Однако в любом случае приложение для гетерогенной платформы должно выполнять следующие действия: 
1. Определить компоненты, из которых состоит гетерогенная система. 
2. Изучить характеристики этих компонентов, чтобы программное обеспечение могло адаптироваться к особенностям различных аппаратных элементов. 
3. Создать блоки инструкций (ядра), которые будут выполняться на платформе. 
4. Настройте и управляйте объектами памяти, участвующими в вычислениях. 
5. Выполняйте ядра в правильном порядке и на нужных компонентах системы. 
6. Соберите окончательные результаты. 
 
Эти шаги выполняются с помощью ряда API-интерфейсов в OpenCL, а также среды программирования для ядер. Мы объясним, как всё это работает, с помощью стратегии «разделяй и властвуй». Мы разделим задачу на следующие модели: 
>
> ***Модель платформы***: высокоуровневое описание гетерогенной системы
> 
> ***Модель выполнения***: абстрактное представление того, как потоки инструкций выполняются на гетерогенной платформе
> 
> ***Модель памяти***: совокупность областей памяти в OpenCL и то, как они взаимодействуют во время вычислений в OpenCL. 
> 
> ***Модели программирования***: высокоуровневые абстракции, которые программист использует при разработке алгоритмов для реализации приложения

# Модель платформы

Модель платформы OpenCL определяет высокоуровневое представление любой неоднородной платформы, используемой с OpenCL. Эта модель показана на рисунке 1.6. Платформа OpenCL всегда включает в себя один хост. Хост взаимодействует с внешней по отношению к программе OpenCL средой, включая ввод-вывод или взаимодействие с пользователем программы.

![[OpenCL_P_6.png]]
рис. 1.6. Модель платформы OpenCL с одним хостом и одним или несколькими устройствами OpenCL. Каждое устройство OpenCL имеет один или несколько вычислительных блоков, каждый из которых имеет один или несколько вычислительных элементов.

Хост-система подключена к одному или нескольким устройствам OpenCL. Устройство — это то, на чём выполняются потоки инструкций (или ядер); таким образом, устройство OpenCL часто называют вычислительным устройством. Устройством может быть центральный процессор, графический процессор, цифровой сигнальный процессор или любой другой процессор, предоставляемый аппаратным обеспечением и поддерживаемый поставщиком OpenCL. 

Устройства OpenCL делятся на вычислительные блоки, которые, в свою очередь, делятся на один или несколько вычислительных элементов (PE). Вычисления на устройстве выполняются в вычислительных элементах. Позже, когда мы поговорим о рабочих группах и модели памяти OpenCL, станет ясно, зачем нужно разделять устройство OpenCL на элементы обработки и вычислительные блоки.

# Модель выполнения

Приложение OpenCL состоит из двух различных частей: программы хоста и набора из одного или нескольких ядер (kernels). Программа хоста выполняется на хосте. OpenCL не определяет детали работы программы хоста, а только то, как она взаимодействует с объектами, определенными внутри OpenCL.  

Ядра выполняются на устройствах OpenCL. Они выполняют основную работу приложения OpenCL. Ядра обычно представляют собой простые функции, которые преобразуют входные объекты памяти в выходные объекты памяти. OpenCL определяет два типа ядер:
>
> **Ядра OpenCL**: функции, написанные на языке программирования OpenCL C и скомпилированные с помощью компилятора OpenCL. Все реализации OpenCL должны поддерживать ядра OpenCL.  
> 
> **Нативные ядра**: функции, созданные вне OpenCL и доступные внутри OpenCL через указатель на функцию. Эти функции могут быть, например, определены в исходном коде хоста или экспортированы из специализированной библиотеки. Стоит отметить, что возможность выполнения нативных ядер является необязательной функциональностью в OpenCL, а их семантика определяется конкретной реализацией.

Модель выполнения OpenCL определяет, как выполняются ядра. Чтобы объяснить это подробно, мы разделим обсуждение на несколько частей. Сначала мы объясним, как отдельное ядро выполняется на устройстве OpenCL. Поскольку вся суть написания приложения OpenCL заключается в выполнении ядер, это понятие является ключевым для понимания OpenCL. Затем мы опишем, как хост определяет контекст для выполнения ядер и как ядра ставятся в очередь на выполнение.

### Как Ядро Выполняется на Устройстве OpenCL

Ядро определяется на хосте. Программа хоста выдает команду, которая отправляет ядро на выполнение на устройстве OpenCL. Когда эта команда выдается хостом, система выполнения OpenCL создает целочисленное индексное пространство. Экземпляр ядра выполняется для каждой точки в этом индексном пространстве. Мы называем каждый экземпляр выполняемого ядра **рабочим элементом (work-item)**, который идентифицируется своими координатами в индексном пространстве. Эти координаты представляют собой **глобальный ID** рабочего элемента.

Таким образом, команда, которая отправляет ядро на выполнение, создает набор рабочих элементов, каждый из которых использует одну и ту же последовательность инструкций, определенную одним ядром. Хотя последовательность инструкций одинакова, поведение каждого рабочего элемента может различаться из-за условных операторов в коде или данных, выбранных через глобальный ID.

Рабочие элементы организованы в **рабочие группы (work-groups)**. Рабочие группы предоставляют более грубую декомпозицию индексного пространства и точно охватывают глобальное индексное пространство. Другими словами, рабочие группы имеют одинаковый размер в соответствующих измерениях, и этот размер равномерно делит глобальный размер в каждом измерении. Рабочим группам присваивается уникальный ID с той же размерностью, что и индексное пространство, используемое для рабочих элементов. Внутри рабочей группы рабочим элементам присваивается уникальный **локальный ID**, так что один рабочий элемент может быть однозначно идентифицирован либо своим глобальным ID, либо комбинацией его локального ID и ID рабочей группы.

Рабочие элементы в заданной рабочей группе выполняются параллельно на обрабатывающих элементах одного вычислительного блока. Это ключевой момент для понимания параллелизма в OpenCL. Реализация может сериализовать выполнение ядер. Она даже может сериализовать выполнение рабочих групп в рамках одного вызова ядра. OpenCL гарантирует только то, что рабочие элементы внутри рабочей группы выполняются параллельно (и совместно используют ресурсы процессора на устройстве). Следовательно, нельзя предполагать, что рабочие группы или вызовы ядер выполняются параллельно. Они действительно часто выполняются параллельно, но разработчик алгоритма не может на это полагаться.

Индексное пространство охватывает диапазон значений в N-измерениях и поэтому называется **NDRange**. В настоящее время N в этом N-мерном индексном пространстве может быть 1, 2 или 3. В программе OpenCL NDRange определяется целочисленным массивом длины N, указывающим размер индексного пространства в каждом измерении. Глобальный и локальный ID каждого рабочего элемента является N-мерным кортежем. В простейшем случае компоненты глобального ID принимают значения в диапазоне от нуля до числа элементов в этом измерении минус один.

Рабочим группам присваиваются ID с использованием аналогичного подхода к тому, который используется для рабочих элементов. Массив длины N определяет количество рабочих групп в каждом измерении. Рабочим элементам назначается рабочая группа, и им присваивается локальный ID с компонентами в диапазоне от нуля до размера рабочей группы в этом измерении минус один. Таким образом, комбинация ID рабочей группы и локального ID внутри рабочей группы однозначно определяет рабочий элемент.

Давайте внимательно разберем различные индексы, подразумеваемые этой моделью, и исследуем, как они связаны. Рассмотрим 2D NDRange. Мы будем использовать строчные буквы **g** для глобального ID рабочего элемента в каждом измерении, обозначенном индексами **x** или **y**. Заглавная буква **G** указывает размер индексного пространства в каждом измерении. Следовательно, каждый рабочий элемент имеет координату (**gx, gy**) в глобальном NDRange индексном пространстве размера (**Gx, Gy**) и принимает значения ё.

Мы делим NDRange индексное пространство на рабочие группы. Следуя описанным выше соглашениям, мы будем использовать строчную букву **w** для ID рабочей группы и заглавную букву **W** для количества рабочих групп в каждом измерении. Измерения снова обозначаются индексами **x** и **y**.

OpenCL требует, чтобы количество рабочих групп в каждом измерении равномерно делило размер NDRange индексного пространства в каждом измерении. Таким образом, все рабочие группы заполнены и имеют одинаковый размер. Этот размер в каждом направлении (**x** и **y** в нашем 2D примере) используется для определения локального индексного пространства для каждого рабочего элемента. Мы будем называть это индексное пространство внутри рабочей группы **локальным индексным пространством**. Следуя нашим соглашениям об использовании заглавных и строчных букв, размер нашего локального индексного пространства в каждом измерении (**x** и **y**) обозначается заглавной буквой **L**, а локальный ID внутри рабочей группы обозначается строчной буквой **l**.

Следовательно, наше NDRange индексное пространство размера **Gx** на **Gy** делится на рабочие группы, проиндексированные в пространстве **Wx-by-Wy** с индексами (**wx, wy**). Каждая рабочая группа имеет размер **Lx** на **Ly**, где мы получаем следующее:
```
Lx = Gx/Wx
Ly = Gy/Wy
```

Мы можем определить рабочий элемент (work-item) с помощью его глобального ID (**gx, gy**) или комбинацией его локального ID (**lx, ly**) и ID рабочей группы (**wx, wy**):
```
gx = wx * Lx + lx
gy = wy * Ly + ly
```

Альтернативно, мы можем двигаться в обратном направлении от **gx** и **gy**, чтобы восстановить локальный ID и ID рабочей группы следующим образом:
```
wx = gx/Lx
wy = gy/Ly

lx = gx % Lx
ly = gy % Ly
```

В этих уравнениях мы использовали целочисленное деление (деление с усечением) и операцию взятия модуля, или «остатка от деления» (%).  

Во всех этих уравнениях мы предполагали, что индексное пространство начинается с нуля в каждом измерении. Однако часто индексы выбираются так, чтобы они соответствовали тем, которые естественны для исходной задачи. Поэтому в OpenCL 1.1 была добавлена возможность определить смещение (offset) для начальной точки глобального индексного пространства. Смещение определяется для каждого измерения (в нашем примере — **x** и **y**), и поскольку оно изменяет глобальный индекс, мы будем использовать строчную букву **o** для обозначения смещения. Таким образом, при ненулевом смещении (**ox, oy**) наше окончательное уравнение, связывающее глобальные и локальные индексы, выглядит следующим образом:
```
gx = wx * Lx + lx + ox
gy = wy * Ly + ly + oy
```

На рисунке 1.7 мы приводим конкретный пример, где каждый маленький квадрат представляет собой рабочий элемент (work-item). В этом примере мы используем нулевое смещение по умолчанию для каждой размерности. Внимательно изучите этот рисунок и убедитесь, что вы понимаете: заштрихованный квадрат с глобальным индексом (6, 5) принадлежит рабочей группе с идентификатором (1, 1) и имеет локальный индекс (2, 1).

![[OpenCL_P_7.png]]
рис. 1.7. Пример того, как глобальные идентификаторы (IDs), локальные идентификаторы и индексы рабочих групп связаны для двумерного NDRange. Другие параметры пространства индексов определены на рисунке. Заштрихованный блок имеет глобальный идентификатор (gx, gy) = (6, 5), идентификатор рабочей группы (wx, wy) = (1, 1) и локальный идентификатор (lx, ly) = (2, 1).

Если все эти манипуляции с индексами кажутся запутанными, не переживайте. Во многих случаях программисты на OpenCL работают только в глобальном пространстве индексов. Со временем, по мере того как вы будете набираться опыта работы с различными типами индексов в OpenCL, такие операции станут для вас второй натурой.

Модель выполнения OpenCL весьма гибкая. Она поддерживает широкий спектр моделей программирования. Однако при разработке OpenCL явно учитывались только две модели: параллелизм данных и параллелизм задач. Мы вернемся к этим моделям и их значению для OpenCL позже. Но сначала нам нужно завершить обзор модели выполнения OpenCL.

# Контекст

Вычислительная работа приложения OpenCL выполняется на устройствах OpenCL. Однако хост играет очень важную роль в приложении OpenCL. Именно на хосте определяются ядра (kernels). Хост устанавливает контекст для ядер. Хост определяет NDRange и очереди, которые контролируют детали того, как и когда ядра выполняются. Все эти важные функции содержатся в API, определенном в OpenCL.

Первая задача хоста — определить контекст для приложения OpenCL. Как следует из названия, контекст определяет среду, в которой ядра определяются и выполняются. Более точно мы определяем контекст через следующие ресурсы:
>
> **Устройства (Devices):** набор устройств OpenCL, которые будут использоваться хостом.
> 
> **Ядра (Kernels):** функции OpenCL, которые выполняются на устройствах OpenCL.
> 
> **Объекты программ (Program objects):** исходный код программы и исполняемые файлы, реализующие ядра.
> 
> **Объекты памяти (Memory objects):** набор объектов в памяти, видимых для устройств OpenCL и содержащих значения, над которыми могут оперировать экземпляры ядра.

Контекст создается и управляется хостом с использованием функций из API OpenCL. Например, рассмотрим гетерогенную платформу с рисунка 1.3. Эта система имеет два многоядерных CPU и GPU. Хост-программа работает на одном из CPU. Хост-программа запросит систему, чтобы обнаружить эти ресурсы, а затем решит, какие устройства использовать в приложении OpenCL. В зависимости от задачи и ядер, которые нужно выполнить, хост может выбрать GPU, другой CPU, другие ядра на том же CPU или любую их комбинацию. После этого выбора определяются устройства OpenCL в текущем контексте.

Также в контекст входят один или несколько объектов программ, содержащих код для ядер. Название "объект программы" может быть немного запутывающим. Лучше думать о них как о динамической библиотеке, из которой вытягиваются функции, используемые ядрами. Объект программы строится во время выполнения в рамках хост-программы. Это может показаться странным для программистов, не входящих в сообщество разработчиков графики. Рассмотрим на минуту вызов, с которым сталкивается программист OpenCL. Он или она пишет приложение OpenCL и передает его конечному пользователю, но этот пользователь может выбрать, где запустить приложение. Программист OpenCL не имеет контроля над тем, какие GPU, CPU или другие чипы конечный пользователь будет использовать. Все, что программист OpenCL знает, — это то, что целевая платформа будет соответствовать спецификации OpenCL.

Решение этой проблемы заключается в том, чтобы объект программы создавался из исходного кода во время выполнения. Хост-программа определяет устройства внутри контекста. Только после этого становится возможным знать, как скомпилировать исходный код программы для создания кода для ядер. Что касается самого исходного кода, OpenCL достаточно гибок в отношении его формы. Во многих случаях это обычная строка, которая либо статически определена в хост-программе, загружается из файла во время выполнения или динамически генерируется внутри хост-программы.

Теперь наш контекст включает устройства OpenCL и объект программы, из которого берутся ядра для выполнения. Далее рассмотрим, как ядра взаимодействуют с памятью. Подробная модель памяти, используемая OpenCL, будет описана позже. Для целей нашего обсуждения контекста нам нужно понять, как работает память OpenCL только на высоком уровне. Суть в том, что на гетерогенной платформе часто существует несколько адресных пространств для управления. У хоста есть знакомое адресное пространство, которое ожидается на платформе CPU, но устройства могут иметь ряд различных архитектур памяти. Чтобы справиться с этой ситуацией, OpenCL вводит концепцию объектов памяти. Они явно определяются на хосте и явно перемещаются между хостом и устройствами OpenCL. Это добавляет дополнительную нагрузку на программиста, но позволяет поддерживать гораздо более широкий спектр платформ.

Теперь мы понимаем контекст внутри приложения OpenCL. Контекст — это устройства OpenCL, объекты программ, ядра и объекты памяти, которые используются ядром при его выполнении. Теперь мы можем перейти к тому, как хост-программа выдает команды устройствам OpenCL.

# Очереди команд

Взаимодействие между хостом и устройствами OpenCL происходит через команды, которые хост помещает в очередь команд. Эти команды ожидают в очереди команд, пока не будут выполнены на устройстве OpenCL. Очередь команд создается хостом и привязывается к одному устройству OpenCL после того, как контекст был определен. Хост помещает команды в очередь команд, а затем они планируются для выполнения на связанном устройстве. OpenCL поддерживает три типа команд:
>
> **Команды выполнения ядер:** выполняют ядро на элементах обработки устройства OpenCL.
> 
> **Команды памяти:** передают данные между хостом и различными объектами памяти, перемещают данные между объектами памяти или отображают (map) и снимают отображение (unmap) объектов памяти из адресного пространства хоста.
> 
> **Команды синхронизации:** накладывают ограничения на порядок выполнения команд.

В типичной хост-программе программист определяет контекст и очереди команд, создает объекты памяти и программы, а также строит любые необходимые структуры данных на хосте для поддержки приложения. Затем фокус переключается на очередь команд. Объекты памяти перемещаются с хоста на устройства; аргументы ядра связываются с объектами памяти, а затем отправляются в очередь команд для выполнения. Когда ядро завершает свою работу, объекты памяти, созданные в процессе вычислений, могут быть скопированы обратно на хост.

Когда несколько ядер отправляются в очередь, они могут нуждаться во взаимодействии. Например, одна группа ядер может создавать объекты памяти, которые следующая группа ядер должна обработать. В этом случае команды синхронизации могут использоваться для принудительного завершения первой группы ядер перед началом выполнения второй группы.

Существует множество дополнительных тонкостей, связанных с тем, как команды работают в OpenCL. Мы оставим эти детали для последующих глав книги. Наша текущая цель — понять очереди команд и получить общее представление о командах OpenCL.

До сих пор мы мало говорили о порядке выполнения команд или о том, как их выполнение связано с выполнением хост-программы. Команды всегда выполняются асинхронно по отношению к хост-программе. Хост-программа отправляет команды в очередь команд и продолжает выполнение, не дожидаясь завершения команд. Если необходимо, чтобы хост дождался завершения команды, это можно явно указать с помощью команды синхронизации.

Команды внутри одной очереди выполняются относительно друг друга в одном из двух режимов:
>
> **Последовательное выполнение (in-order execution):** команды запускаются в том порядке, в котором они появляются в очереди команд, и завершаются в том же порядке. Другими словами, предыдущая команда в очереди завершается до того, как начнется следующая. Это сериализует порядок выполнения команд в очереди.
  >
> **Непоследовательное выполнение (out-of-order execution):** команды выдаются в порядке поступления, но не ждут завершения перед выполнением следующих команд. Любые ограничения на порядок выполнения обеспечиваются программистом через явные механизмы синхронизации.

Все платформы OpenCL поддерживают режим последовательного выполнения, но режим непоследовательного выполнения является необязательным. Зачем использовать режим непоследовательного выполнения? Рассмотрим рисунок 1.5, где мы ввели концепцию балансировки нагрузки. Приложение считается завершенным только тогда, когда все ядра завершат выполнение. Следовательно, для эффективной программы, минимизирующей время выполнения, вы хотите, чтобы все вычислительные блоки были полностью задействованы и работали примерно одинаковое количество времени. Часто этого можно достичь, тщательно продумывая порядок отправки команд в очереди, чтобы последовательное выполнение обеспечивало хорошо сбалансированную нагрузку. Однако, когда у вас есть набор команд, которые требуют разное время выполнения, балансировка нагрузки так, чтобы все вычислительные блоки оставались полностью загруженными и завершались одновременно, может быть сложной задачей. Очередь с непоследовательным выполнением может решить эту проблему за вас. Команды могут выполняться в любом порядке, поэтому если вычислительный блок завершает свою работу раньше, он может немедленно получить новую команду из очереди команд и начать выполнение нового ядра. Это называется автоматической балансировкой нагрузки и является хорошо известной техникой, используемой в проектировании параллельных алгоритмов, управляемых очередями команд ( #см_шаблон Master-Worker в книге Т. Г. Мэттсона и др., "Patterns for Parallel Programming").

Всякий раз, когда в приложении происходят множественные параллельные выполнения, существует потенциальная опасность. Данные могут быть случайно использованы до того, как они были записаны, или ядра могут выполняться в порядке, который приводит к неверным результатам. Программисту нужен способ управлять ограничениями на команды. Мы уже упомянули один из них — команду синхронизации, которая заставляет группу ядер ждать завершения предыдущей группы. Это часто весьма эффективно, но иногда требуются более сложные протоколы синхронизации.

Для поддержки пользовательских протоколов синхронизации команды, отправленные в очередь команд, генерируют объекты событий. Команде можно сказать, чтобы она ждала, пока не будут выполнены определенные условия на объектах событий. Эти события также могут использоваться для координации выполнения между хостом и устройствами OpenCL. Мы расскажем об этих событиях позже.

Наконец, возможно связать несколько очередей с одним контекстом для любого из устройств OpenCL в этом контексте. Эти две очереди выполняются параллельно и независимо, без явных механизмов в OpenCL для их синхронизации.

# Модель памяти

Модель выполнения объясняет, как выполняются ядра, как они взаимодействуют с хостом и как они взаимодействуют с другими ядрами. Для описания этой модели и связанной с ней очереди команд мы кратко упомянули объекты памяти. Однако мы не определили детали этих объектов — ни типы объектов памяти, ни правила их безопасного использования. Эти вопросы охватываются моделью памяти OpenCL.

OpenCL определяет два типа объектов памяти: **буферные объекты** и **объекты изображений**. Буферный объект, как следует из названия, представляет собой непрерывный блок памяти, доступный для ядер. Программист может отобразить структуры данных на этот буфер и обращаться к нему через указатели. Это предоставляет гибкость для определения практически любых структур данных, которые программист желает (в рамках ограничений языка программирования ядер OpenCL).

Объекты изображений, напротив, предназначены исключительно для хранения изображений. Формат хранения изображений может быть оптимизирован под потребности конкретного устройства OpenCL. Поэтому важно, чтобы OpenCL предоставлял реализации свободу настраивать формат изображений. Таким образом, объект памяти изображения является непрозрачным. Фреймворк OpenCL предоставляет функции для работы с изображениями, но за исключением этих специфических функций, содержимое объекта изображения скрыто от программы ядра.

OpenCL также позволяет программисту указывать подрегионы объектов памяти как отдельные объекты памяти (эта возможность была добавлена в спецификации OpenCL 1.1). Это делает подрегион большого объекта памяти полноценным объектом в OpenCL, который можно манипулировать и координировать через очередь команд.

Понимание самих объектов памяти — это только первый шаг. Нам также нужно понять конкретные абстракции, которые регулируют их использование в программе OpenCL. Модель памяти OpenCL определяет пять различных областей памяти:

- **Память хоста:** Этот регион памяти виден только хосту. Как и большинство деталей, касающихся хоста, OpenCL определяет только то, как память хоста взаимодействует с объектами и конструкциями OpenCL.
  
- **Глобальная память:** Этот регион памяти позволяет всем рабочим элементам во всех рабочих группах осуществлять операции чтения/записи. Рабочие элементы могут читать или записывать любые элементы объекта памяти в глобальной памяти. Чтение и запись в глобальную память могут кэшироваться в зависимости от возможностей устройства.

- **Константная память:** Этот регион глобальной памяти остается постоянным во время выполнения ядра. Хост выделяет и инициализирует объекты памяти, помещенные в константную память. Рабочие элементы имеют доступ только для чтения к этим объектам.

- **Локальная память:** Этот регион памяти локален для рабочей группы. Этот регион памяти может использоваться для выделения переменных, которые являются общими для всех рабочих элементов в этой рабочей группе. Он может быть реализован как выделенные области памяти на устройстве OpenCL. В качестве альтернативы, регион локальной памяти может быть отображен на секции глобальной памяти.

- **Приватная память:** Этот регион памяти является приватным для рабочего элемента. Переменные, определенные в приватной памяти одного рабочего элемента, не видны другим рабочим элементам.

Регионы памяти и то, как они связаны с платформой и моделью выполнения, описаны на рисунке 1.8. Рабочие элементы выполняются на процессорных элементах (PE) и имеют свою собственную приватную память. Рабочая группа выполняется на вычислительном блоке и разделяет регион локальной памяти с рабочими элементами в группе. Память устройства OpenCL работает с хостом для поддержки глобальной памяти. Модели памяти хоста и устройства OpenCL в основном независимы друг от друга. Это необходимо, учитывая, что хост определен вне OpenCL. Однако иногда им нужно взаимодействовать. Это взаимодействие происходит одним из двух способов: либо путем явного копирования данных, либо путем отображения и снятия отображения регионов объекта памяти.

Для явного копирования данных хост ставит в очередь команды для передачи данных между объектом памяти и памятью хоста. Команды передачи памяти могут быть блокирующими или неблокирующими. Вызов функции OpenCL для блокирующей передачи памяти возвращает управление только после того, как связанные ресурсы памяти на хосте можно безопасно использовать повторно. Для неблокирующей передачи памяти вызов функции OpenCL возвращает управление сразу после постановки команды в очередь, независимо от того, безопасно ли использовать память хоста.

Метод взаимодействия между хостом и объектами памяти OpenCL посредством отображения/снятия отображения позволяет хосту отобразить регион из объекта памяти в свое собственное адресное пространство. Команда отображения памяти (которая ставится в очередь команд как любая другая команда OpenCL) может быть блокирующей или неблокирующей. После того как регион из объекта памяти отображен, хост может читать или записывать в этот регион. Хост снимает отображение региона, когда доступ (чтение и/или запись) к этому отображенному региону со стороны хоста завершен.

![[image/OpenCL_P_8.png]]
Рисунок 1.8. Краткое описание модель памяти в OpenCL и как различные области памяти взаимодействуют с моделью платформы














