
OpenCL язык и API
1. [[#Введение в OpenCL]] 1.1
2. [[#Что такое OpenCL, или ... зачем вам нужна эта книга]] 1.2
3. [[#Наше многоядерное будущее гетерогенные платформы.|Наше многоядерное будущее: гетерогенные платформы.]] 1.3
4. [[#Программное обеспечение в многоядерном мире.|Программное обеспечение в многоядерном мире.]] 1.4
5. [[#Концептуальные основы OpenCL|Концептуальные основы OpenCL]] 1.5
6. [[#Модель платформы|Модель платформы]] 1.6
7. [[#Модель выполнения|Модель выполнения]] 1.7

# Введение в OpenCL

При изучении новой модели программирования легко потеряться в море деталей. API и странная новая терминология, казалось бы, появляются из ниоткуда, создавая ненужную сложность и запутывая. Главное — начать с чёткого понимания на высоком уровне, чтобы иметь представление, к которому можно обратиться, когда станет трудно. Цель этой главы — помочь вам составить такое представление. Мы начнём с краткого обзора спецификации OpenCL 1.1 и тенденций в области гетерогенных вычислений, которые делают её таким важным стандартом программирования. Затем мы опишем концептуальные модели, лежащие в основе OpenCL, и используем их для объяснения принципов работы OpenCL. На этом этапе мы закладываем теоретическую основу OpenCL и переходим к рассмотрению компонентов OpenCL. Ключевым моментом является то, как OpenCL работает с графическими стандартами. Мы завершим обзор OpenCL кратким рассмотрением того, как стандарт OpenCL работает со встроенными процессорами.

# Что такое OpenCL, или ... зачем вам нужна эта книга

OpenCL — это отраслевой стандарт для программирования компьютеров, состоящих из комбинации центральных процессоров, графических процессоров и других процессоров. Эти так называемые гетерогенные системы стали важным классом платформ, и OpenCL — первый отраслевой стандарт, напрямую отвечающий их потребностям. OpenCL — относительно новая технология, впервые представленная в декабре 2008 года, а первые продукты появились осенью 2009 года. С помощью OpenCL можно написать одну программу, которая будет работать на самых разных системах: от мобильных телефонов до ноутбуков и узлов в огромных суперкомпьютерах. Ни один другой стандарт параллельного программирования не имеет такого широкого охвата. Это одна из причин, почему OpenCL так важен и может изменить индустрию программного обеспечения. Это также источник большей части критики в адрес OpenCL.

OpenCL обеспечивает высокий уровень переносимости, предоставляя доступ к аппаратному обеспечению, а не скрывая его за элегантными абстракциями. Это означает, что программист OpenCL должен явно определять платформу, её контекст и то, как работа распределяется между различными устройствами. Не всем программистам нужен или даже желателен детальный контроль, который обеспечивает OpenCL. И это нормально; если он доступен, то высокоуровневая модель программирования часто является лучшим подходом. Однако даже высокоуровневым моделям программирования нужна прочная (и переносимая) основа, и OpenCL может стать такой основой. Эта книга представляет собой подробное введение в OpenCL. Хотя любой может скачать спецификацию (www.khronos.org/opencl) и выучить все конструкции OpenCL, в спецификации не описано, как использовать OpenCL для решения задач. В этом и заключается смысл этой книги: решение задач с помощью фреймворка OpenCL.

# Наше многоядерное будущее: гетерогенные платформы.

За последнее десятилетие компьютеры претерпели фундаментальные изменения. Раньше инновации зависели от чистой производительности. Однако несколько лет назад акцент сместился на производительность, приходящуюся на один ватт потребляемой энергии. Полупроводниковые компании будут продолжать размещать всё больше и больше транзисторов на одном кристалле, но эти производители будут конкурировать за энергоэффективность, а не за чистую производительность. Этот сдвиг радикально изменил компьютеры, которые производит отрасль. Во-первых, микропроцессоры внутри наших компьютеров состоят из нескольких маломощных ядер. Многоядерный императив был впервые изложен А. П. Чан-дракасаном и др. в статье “Оптимизация мощности с использованием преобразований”.1 Суть их аргументации можно найти на рисунке 1.1. Энергия, затрачиваемая на переключение вентилей в процессоре, равна емкости (C), умноженной на квадрат напряжения (V). Эти вентили переключаются в течение секунды количество раз, равное частоте. Следовательно, мощность микропроцессора измеряется как P = CV2f. Если мы сравним одноядерный процессор, работающий на частоте f и напряжении V, с аналогичным процессором с двумя ядрами, каждое из которых работает на частоте f/2, то мы увеличим количество схем в чипе. Согласно моделям, описанным в разделе «Оптимизация энергопотребления с помощью преобразований», номинально это увеличивает ёмкость в 2,2 раза. Но напряжение существенно падает до 0,6 В. Таким образом, количество выполненных инструкций  в секунду одинаково, но энергопотребление двухъядерного варианта составляет 0,396 от энергопотребления одноядерного. Именно эта фундаментальная взаимосвязь лежит в основе перехода к многоядерным чипам. Множество ядер, работающих на более низких частотах, принципиально более энергоэффективны.
![[OpenCL_P_1.png]]

рис. 1.1. В этих двух случаях скорость выполнения инструкций одинакова, но мощность намного меньше при использовании двух ядер, работающих на половинной частоте одного ядра.

Следующий вопрос: «Будут ли эти ядра одинаковыми (однородными) или они будут отличаться друг от друга?» Чтобы понять эту тенденцию, рассмотрим энергоэффективность специализированной логики по сравнению с логикой общего назначения. Процессор общего назначения по своей природе должен включать в себя широкий спектр функциональных блоков, чтобы удовлетворять любые вычислительные потребности. Именно это делает чип процессором общего назначения. Однако в процессорах, специализированных для выполнения определённой функции, меньше лишних транзисторов, поскольку они включают только те функциональные блоки, которые требуются для выполнения этой функции. Результат можно увидеть на рисунке 1.2, где мы сравниваем процессор общего назначения (Intel Core 2 Quad, модель Q6700)2, графический процессор (NVIDIA GTX 280) и узкоспециализированный исследовательский процессор (Intel 80-ядерный исследовательский процессор Tera-scale, ядра которого представляют собой просто пару арифметических блоков умножения-суммирования с плавающей запятой). Чтобы сравнение было максимально объективным, каждый из чипов был изготовлен по техпроцессу 65 нм, и мы использовали опубликованную производителем максимальную производительность в зависимости от расчётной тепловой мощности. Как видно на рисунке, чем более специализирован процессор, тем выше его энергоэффективность, если задачи хорошо подходят для него.

![[OpenCL_P_2.png]]
рис. 1.2. График зависимости пиковой производительности от мощности в точке теплового проектирования для трёх процессоров, произведённых по 65-нм техпроцессу. Примечание: это не значит, что один процессор лучше или хуже других. Дело в том, что чем более специализированно ядро, тем оно энергоэффективнее.

Таким образом, есть все основания полагать, что в мире, где важна максимальная производительность на ватт, мы можем ожидать, что системы будут всё больше зависеть от множества ядер со специализированным кремнием, где это возможно. Это особенно важно для мобильных устройств, в которых критически важно экономить заряд батареи. Однако это неоднородное будущее уже наступило. Рассмотрим схематическое изображение современного ПК на рисунке 1.3. Здесь есть два разъёма, в каждом из которых потенциально может находиться многоядерный процессор; контроллер графики и памяти (GMCH), который подключается к системной памяти (DRAM); и графический процессор (GPU). Это гетерогенная платформа с несколькими наборами команд и несколькими уровнями параллелизма, которые необходимо использовать, чтобы раскрыть весь потенциал системы.

![[OpenCL_P_3.png]]
рис. 1.3. Блок-схема современного настольного ПК с несколькими процессорами (потенциально разными) и графическим процессором, демонстрирующая, что современные системы часто бывают неоднородными

Базовая платформа как сегодня, так и в будущем, на высоком уровне ясна. Множество деталей и инноваций, несомненно, удивят нас, но тенденции в области аппаратного обеспечения очевидны. Будущее принадлежит гетерогенным многоядерным платформам. Вопрос, который стоит перед нами, заключается в том, как наше программное обеспечение должно адаптироваться к этим платформам.

# Программное обеспечение в многоядерном мире.

Параллельное аппаратное обеспечение обеспечивает производительность за счёт одновременного выполнения нескольких операций. Чтобы быть полезным, параллельное аппаратное обеспечение нуждается в программном обеспечении, которое выполняет несколько потоков операций одновременно; другими словами, вам нужно параллельное программное обеспечение. 

Чтобы понять, что такое параллельное программное обеспечение, мы должны начать с более общей концепции параллелизма. ***Параллелизм*** — это старая и хорошо знакомая концепция в информатике. Программная система является параллельной, если она состоит из нескольких потоков операций, которые активны и могут выполняться одновременно. Параллелизм является основополагающим принципом любой современной операционной системы. Он позволяет максимально эффективно использовать ресурсы, позволяя другим потокам операций (нитям) выполняться, пока другие приостановлены в ожидании какого-либо ресурса. Он создаёт у пользователя, взаимодействующего с системой, иллюзию непрерывного и почти мгновенного взаимодействия с системой.

Когда параллельное программное обеспечение работает на компьютере с несколькими процессорами, так что потоки фактически выполняются одновременно, мы имеем дело с параллельными вычислениями. Параллелизм, обеспечиваемый аппаратным обеспечением, — это одновременная работа нескольких процессов. 

Задача программистов — найти параллелизм в своей задаче, выразить его в программном обеспечении, а затем запустить полученную программу так, чтобы параллелизм обеспечивал желаемую производительность. Найти параллелизм в задаче может быть так же просто, как выполнить независимый поток операций для каждого пикселя изображения. Или это может быть невероятно сложно из-за множества потоков операций, которые обмениваются информацией и должны тесно взаимодействовать при выполнении.

Как только в задаче обнаруживается параллелизм, программисты должны выразить этот параллелизм в исходном коде. В частности, необходимо определить потоки операций, которые будут выполняться параллельно, связать с ними данные, с которыми они работают, и управлять зависимостями между ними, чтобы при их параллельном выполнении получался правильный ответ. В этом и заключается суть проблемы параллельного программирования. 

Манипулирование низкоуровневыми деталями параллельного компьютера находится за пределами возможностей большинства людей. Даже опытные программисты, работающие с параллельными вычислениями, были бы перегружены необходимостью управлять каждым конфликтом памяти или планировать отдельные потоки. Следовательно, ключом к параллельному программированию является высокоуровневая абстракция или модель, которая делает задачу параллельного программирования более управляемой.

Существует слишком много моделей программирования, разделённых на пересекающиеся категории с запутанными и часто неоднозначными названиями. Для наших целей мы рассмотрим две модели параллельного программирования: параллелизм задач и параллелизм данных. На высоком уровне идеи, лежащие в основе этих двух моделей, просты.

В модели программирования с параллельной обработкой данных программисты рассматривают свои задачи как наборы элементов данных, которые можно обновлять одновременно. Параллелизм выражается в одновременном применении одного и того же потока инструкций (задачи) к каждому элементу данных. Параллелизм заключается в данных. На рисунке 1.4 приведён простой пример параллельной обработки данных. Рассмотрим простую задачу, которая просто возвращает квадрат входного значения и вектор чисел (`A_vector`). Используя модель программирования с параллельной обработкой данных, мы обновляем вектор параллельно, определяя, что задача должна быть применена к каждому элементу для получения нового результирующего вектора. Конечно, этот пример очень простой. На практике количество операций в задаче должно быть большим, чтобы компенсировать затраты на перемещение данных и управлять параллельными вычислениями. Но простой пример на рисунке отражает ключевую идею этого режима программирования.

![[OpenCL_P_4.png]]
Рис. 1.4. Простой пример параллельной обработки данных, когда к каждому элементу вектора одновременно применяется одна задача для создания нового вектора

В модели программирования с разделением задач на параллельные потоки программисты напрямую определяют и управляют параллельными задачами. Проблемы разбиваются на задачи, которые могут выполняться параллельно, а затем распределяются по вычислительным элементам (PE) параллельного компьютера для выполнения. Это проще всего сделать, когда задачи полностью независимы, но эта модель программирования также используется для задач, которые используют общие данные. Вычисления с набором задач завершаются, когда выполняется последняя задача. Поскольку задачи сильно различаются по вычислительным требованиям, их распределение таким образом, чтобы все они выполнялись примерно в одно и то же время, может быть затруднительным. Это проблема балансировки нагрузки. Рассмотрим пример на рисунке 1.5, где у нас есть шесть независимых задач, которые нужно выполнить одновременно на трёх PE (процессорный элемент). В одном случае у первого процессорного элемента есть дополнительная работа, и он работает значительно дольше, чем другие процессорные элементы. Во втором случае с другим распределением задач показан более идеальный вариант, при котором каждый процессорный элемент завершает работу примерно в одно и то же время. Это пример ключевого идеала параллельных вычислений, называемого ***балансировкой нагрузки***.

Выбор между параллелизмом данных и параллелизмом задач зависит от потребностей решаемой задачи. Например, задачи, связанные с обновлением точек на сетке, сразу же можно решить с помощью моделей параллелизма данных. С другой стороны, задачи, связанные с обходом графов, естественным образом решаются с помощью параллелизма задач. Следовательно, программисту, работающему с параллельными вычислениями, необходимо разбираться в обеих моделях программирования. А общая платформа программирования (например, OpenCL) должна поддерживать обе модели.

Независимо от модели программирования, следующим шагом в процессе параллельного программирования является привязка программы к реальному оборудованию. Именно на этом этапе возникают уникальные проблемы, связанные с разнородными компьютерами. Вычислительные элементы в системе могут иметь разные наборы команд и архитектуры памяти и работать с разной скоростью. Эффективная программа должна учитывать эти различия и соответствующим образом привязывать параллельное программное обеспечение к наиболее подходящим устройствам OpenCL.

![[OpenCL_P_5.png]]
рис. 1.5. Параллелизм задач, демонстрирующий два способа распределения шести независимых задач между тремя исполнительными элементами. Вычисления не выполняются до тех пор, пока не будет завершена каждая задача, поэтому целью должна быть сбалансированная нагрузка, то есть одинаковое время выполнения вычислений каждым исполнительным элементом.

Традиционно программисты решали эту проблему, рассматривая своё программное обеспечение как набор модулей, реализующих отдельные части задачи. Модули напрямую связаны с компонентами гетерогенной платформы. Например, графическое программное обеспечение работает на графическом процессоре. Другое программное обеспечение работает на центральном процессоре.

Программирование графических процессоров общего назначения (GPGPU) нарушило эту модель. Алгоритмы, не связанные с графикой, были модифицированы для работы на графическом процессоре. Центральный процессор настраивает вычисления и управляет вводом-выводом, но все «интересные» вычисления переносятся на графический процессор. По сути, гетерогенная платформа игнорируется, а основное внимание уделяется одному компоненту системы — графическому процессору.

OpenCL не рекомендует использовать такой подход. По сути, пользователь «платит за все устройства OpenCL» в системе, поэтому эффективная программа должна использовать их все. Именно это OpenCL рекомендует делать программистам и чего вы ожидаете от среды программирования, разработанной для гетерогенных платформ.

Аппаратная неоднородность сложна. Программисты привыкли полагаться на высокоуровневые абстракции, которые скрывают сложность аппаратного обеспечения. Неоднородный язык программирования выявляет неоднородность и противоречит тенденции к повышению уровня абстракции.

# Концептуальные основы OpenCL

Как мы увидим далее в этой книге, OpenCL поддерживает широкий спектр приложений. Трудно сделать какие-либо обобщения об этих приложениях. Однако в любом случае приложение для гетерогенной платформы должно выполнять следующие действия: 
1. Определить компоненты, из которых состоит гетерогенная система. 
2. Изучить характеристики этих компонентов, чтобы программное обеспечение могло адаптироваться к особенностям различных аппаратных элементов. 
3. Создать блоки инструкций (ядра), которые будут выполняться на платформе. 
4. Настройте и управляйте объектами памяти, участвующими в вычислениях. 
5. Выполняйте ядра в правильном порядке и на нужных компонентах системы. 
6. Соберите окончательные результаты. 
 
Эти шаги выполняются с помощью ряда API-интерфейсов в OpenCL, а также среды программирования для ядер. Мы объясним, как всё это работает, с помощью стратегии «разделяй и властвуй». Мы разделим задачу на следующие модели: 
>
> ***Модель платформы***: высокоуровневое описание гетерогенной системы
> 
> ***Модель выполнения***: абстрактное представление того, как потоки инструкций выполняются на гетерогенной платформе
> 
> ***Модель памяти***: совокупность областей памяти в OpenCL и то, как они взаимодействуют во время вычислений в OpenCL. 
> 
> ***Модели программирования***: высокоуровневые абстракции, которые программист использует при разработке алгоритмов для реализации приложения

# Модель платформы

Модель платформы OpenCL определяет высокоуровневое представление любой неоднородной платформы, используемой с OpenCL. Эта модель показана на рисунке 1.6. Платформа OpenCL всегда включает в себя один хост. Хост взаимодействует с внешней по отношению к программе OpenCL средой, включая ввод-вывод или взаимодействие с пользователем программы.

![[OpenCL_P_6.png]]
рис. 1.6. Модель платформы OpenCL с одним хостом и одним или несколькими устройствами OpenCL. Каждое устройство OpenCL имеет один или несколько вычислительных блоков, каждый из которых имеет один или несколько вычислительных элементов.

Хост-система подключена к одному или нескольким устройствам OpenCL. Устройство — это то, на чём выполняются потоки инструкций (или ядер); таким образом, устройство OpenCL часто называют вычислительным устройством. Устройством может быть центральный процессор, графический процессор, цифровой сигнальный процессор или любой другой процессор, предоставляемый аппаратным обеспечением и поддерживаемый поставщиком OpenCL. 

Устройства OpenCL делятся на вычислительные блоки, которые, в свою очередь, делятся на один или несколько вычислительных элементов (PE). Вычисления на устройстве выполняются в вычислительных элементах. Позже, когда мы поговорим о рабочих группах и модели памяти OpenCL, станет ясно, зачем нужно разделять устройство OpenCL на элементы обработки и вычислительные блоки.

# Модель выполнения

Приложение OpenCL состоит из двух различных частей: программы хоста и набора из одного или нескольких ядер (kernels). Программа хоста выполняется на хосте. OpenCL не определяет детали работы программы хоста, а только то, как она взаимодействует с объектами, определенными внутри OpenCL.  

Ядра выполняются на устройствах OpenCL. Они выполняют основную работу приложения OpenCL. Ядра обычно представляют собой простые функции, которые преобразуют входные объекты памяти в выходные объекты памяти. OpenCL определяет два типа ядер:
>
> **Ядра OpenCL**: функции, написанные на языке программирования OpenCL C и скомпилированные с помощью компилятора OpenCL. Все реализации OpenCL должны поддерживать ядра OpenCL.  
> 
> **Нативные ядра**: функции, созданные вне OpenCL и доступные внутри OpenCL через указатель на функцию. Эти функции могут быть, например, определены в исходном коде хоста или экспортированы из специализированной библиотеки. Стоит отметить, что возможность выполнения нативных ядер является необязательной функциональностью в OpenCL, а их семантика определяется конкретной реализацией.

Модель выполнения OpenCL определяет, как выполняются ядра. Чтобы объяснить это подробно, мы разделим обсуждение на несколько частей. Сначала мы объясним, как отдельное ядро выполняется на устройстве OpenCL. Поскольку вся суть написания приложения OpenCL заключается в выполнении ядер, это понятие является ключевым для понимания OpenCL. Затем мы опишем, как хост определяет контекст для выполнения ядер и как ядра ставятся в очередь на выполнение.

### Как Ядро Выполняется на Устройстве OpenCL

Ядро определяется на хосте. Программа хоста выдает команду, которая отправляет ядро на выполнение на устройстве OpenCL. Когда эта команда выдается хостом, система выполнения OpenCL создает целочисленное индексное пространство. Экземпляр ядра выполняется для каждой точки в этом индексном пространстве. Мы называем каждый экземпляр выполняемого ядра **рабочим элементом (work-item)**, который идентифицируется своими координатами в индексном пространстве. Эти координаты представляют собой **глобальный ID** рабочего элемента.

Таким образом, команда, которая отправляет ядро на выполнение, создает набор рабочих элементов, каждый из которых использует одну и ту же последовательность инструкций, определенную одним ядром. Хотя последовательность инструкций одинакова, поведение каждого рабочего элемента может различаться из-за условных операторов в коде или данных, выбранных через глобальный ID.

Рабочие элементы организованы в **рабочие группы (work-groups)**. Рабочие группы предоставляют более грубую декомпозицию индексного пространства и точно охватывают глобальное индексное пространство. Другими словами, рабочие группы имеют одинаковый размер в соответствующих измерениях, и этот размер равномерно делит глобальный размер в каждом измерении. Рабочим группам присваивается уникальный ID с той же размерностью, что и индексное пространство, используемое для рабочих элементов. Внутри рабочей группы рабочим элементам присваивается уникальный **локальный ID**, так что один рабочий элемент может быть однозначно идентифицирован либо своим глобальным ID, либо комбинацией его локального ID и ID рабочей группы.

Рабочие элементы в заданной рабочей группе выполняются параллельно на обрабатывающих элементах одного вычислительного блока. Это ключевой момент для понимания параллелизма в OpenCL. Реализация может сериализовать выполнение ядер. Она даже может сериализовать выполнение рабочих групп в рамках одного вызова ядра. OpenCL гарантирует только то, что рабочие элементы внутри рабочей группы выполняются параллельно (и совместно используют ресурсы процессора на устройстве). Следовательно, нельзя предполагать, что рабочие группы или вызовы ядер выполняются параллельно. Они действительно часто выполняются параллельно, но разработчик алгоритма не может на это полагаться.

Индексное пространство охватывает диапазон значений в N-измерениях и поэтому называется **NDRange**. В настоящее время N в этом N-мерном индексном пространстве может быть 1, 2 или 3. В программе OpenCL NDRange определяется целочисленным массивом длины N, указывающим размер индексного пространства в каждом измерении. Глобальный и локальный ID каждого рабочего элемента является N-мерным кортежем. В простейшем случае компоненты глобального ID принимают значения в диапазоне от нуля до числа элементов в этом измерении минус один.

Рабочим группам присваиваются ID с использованием аналогичного подхода к тому, который используется для рабочих элементов. Массив длины N определяет количество рабочих групп в каждом измерении. Рабочим элементам назначается рабочая группа, и им присваивается локальный ID с компонентами в диапазоне от нуля до размера рабочей группы в этом измерении минус один. Таким образом, комбинация ID рабочей группы и локального ID внутри рабочей группы однозначно определяет рабочий элемент.

Давайте внимательно разберем различные индексы, подразумеваемые этой моделью, и исследуем, как они связаны. Рассмотрим 2D NDRange. Мы будем использовать строчные буквы **g** для глобального ID рабочего элемента в каждом измерении, обозначенном индексами **x** или **y**. Заглавная буква **G** указывает размер индексного пространства в каждом измерении. Следовательно, каждый рабочий элемент имеет координату (**gx, gy**) в глобальном NDRange индексном пространстве размера (**Gx, Gy**) и принимает значения ё.

Мы делим NDRange индексное пространство на рабочие группы. Следуя описанным выше соглашениям, мы будем использовать строчную букву **w** для ID рабочей группы и заглавную букву **W** для количества рабочих групп в каждом измерении. Измерения снова обозначаются индексами **x** и **y**.

OpenCL требует, чтобы количество рабочих групп в каждом измерении равномерно делило размер NDRange индексного пространства в каждом измерении. Таким образом, все рабочие группы заполнены и имеют одинаковый размер. Этот размер в каждом направлении (**x** и **y** в нашем 2D примере) используется для определения локального индексного пространства для каждого рабочего элемента. Мы будем называть это индексное пространство внутри рабочей группы **локальным индексным пространством**. Следуя нашим соглашениям об использовании заглавных и строчных букв, размер нашего локального индексного пространства в каждом измерении (**x** и **y**) обозначается заглавной буквой **L**, а локальный ID внутри рабочей группы обозначается строчной буквой **l**.

Следовательно, наше NDRange индексное пространство размера **Gx** на **Gy** делится на рабочие группы, проиндексированные в пространстве **Wx-by-Wy** с индексами (**wx, wy**). Каждая рабочая группа имеет размер **Lx** на **Ly**, где мы получаем следующее:
```
Lx = Gx/Wx
Ly = Gy/Wy
```

Мы можем определить рабочий элемент (work-item) с помощью его глобального ID (**gx, gy**) или комбинацией его локального ID (**lx, ly**) и ID рабочей группы (**wx, wy**):
```
gx = wx * Lx + lx
gy = wy * Ly + ly
```

Альтернативно, мы можем двигаться в обратном направлении от **gx** и **gy**, чтобы восстановить локальный ID и ID рабочей группы следующим образом:
```
wx = gx/Lx
wy = gy/Ly

lx = gx % Lx
ly = gy % Ly
```

В этих уравнениях мы использовали целочисленное деление (деление с усечением) и операцию взятия модуля, или «остатка от деления» (%).  

Во всех этих уравнениях мы предполагали, что индексное пространство начинается с нуля в каждом измерении. Однако часто индексы выбираются так, чтобы они соответствовали тем, которые естественны для исходной задачи. Поэтому в OpenCL 1.1 была добавлена возможность определить смещение (offset) для начальной точки глобального индексного пространства. Смещение определяется для каждого измерения (в нашем примере — **x** и **y**), и поскольку оно изменяет глобальный индекс, мы будем использовать строчную букву **o** для обозначения смещения. Таким образом, при ненулевом смещении (**ox, oy**) наше окончательное уравнение, связывающее глобальные и локальные индексы, выглядит следующим образом:
```
gx = wx * Lx + lx + ox
gy = wy * Ly + ly + oy
```










