# Параллельные вычисления

Одним из наибольших триумфов С++11 является включение параллелизма в язык программирования и библиотеку. Программисты, знакомые с другими потоковыми API (например, `pthreads` или `Windows threads`), иногда удивляются сравнительно спартанскому набору возможностей, предлагаемому С++, но это связано с тем, что по большей части поддержка параллельности в С++ имеет вид ограничений для разработчиков компиляторов. Получаемые в результате языковые гарантии означают, что впервые в истории С++ программисты могут писать многопоточные приложения со стандартным поведением на всех платформах. Это создает прочный фундамент, на котором могут быть построены
выразительные библиотеки, а элементы параллелизма в стандартной библиотеке (задачи, фьючерсы, потоки, мьютексы, переменные условий, атомарные объекты и прочие) являются лишь началом того, что обязательно станет более богатым набором инструментов для разработки параллельного программного обеспечения на С++.

В последующих разделах нужно иметь в виду, что стандартная библиотека содержит два шаблона для [[future|фьючерсов:]] и [[future#std shared_future|std::shared_future]]. Во многих случаях различия между ними не важны, так что я часто говорю просто о фьючерсах, подразумевая при этом обе разновидности.

## Предпочитайте программирование на основе задач программированию на основе потоков

Если вы хотите выполнить функцию `doAsyncWork` асинхронно, у вас есть два основных варианта. Вы можете создать [[thread#std::thread|std::thread]] и запустить с его помощью `doAsyncWork`, тем самым прибегнув к подходу на основе потоков:

```c++
int doAsyncWork();
std::thread t(doAsyncWork);
```

Вы также можете передать `doAsyncWork` в [[async|std::async]], воспользовавшись стратегией, известной как подход на основе задач:

```c++
auto fut = std::async(doAsyncWork);            // " fut" от " future"
```

В таких вызовах функциональный объект, переданный в [[async|std::async]] (например,
`doAsyncWork`) , рассматривается как задача ([[task|task]]).

Подход на основе задач обычно превосходит свой аналог на основе потоков, и небольшие фрагменты кода, с которыми вы встретились выше, показывают некоторые причины этого. Здесь `doAsyncwork` дает возвращаемое значение, в котором, как мы можем разумно предположить, заинтересован вызывающий `doAsyncwork` код. В случае вызова на основе потоков нет простого способа к нему обратиться. При подходе на основе потоков нет простого способа получить доступ к вызову. В случае же подхода на основе задач это можно легко сделать, поскольку фьючерс, возвращаемый [[async|std::async]], предлагает функцию `get`. Эта функция `get` еще более важна, если `doAsyncwork` генерирует исключение, поскольку `get` обеспечивает доступ и к нему. При подходе на основе потоков в случае генерации функцией `doAsyncWork` исключения программа аварийно завершается (с помощью вызова [[terminate|std::terminate]]).

Более фундаментальным различием между подходами на основе потоков и на основе задач является воплощение более высокого уровня абстракции в последнем. Он освобождает вас от деталей управления потоками, что, кстати, напомнило мне о необходимости рассказать о трех значениях слова поток ([[Parallel programming#Базовые потоки типа std thread|thread]]) в параллельном программировании на С++.

> **Аппаратные потоки** являются потоками, которые выполняют фактические вычисления. Современные машинные архитектуры предлагают по одному или по нескольку аппаратных потоков для каждого ядра процессора.
> 
> **Программные потоки** (известные также как потоки ОС или системные потоки) являются потоками, управляемыми операционной системой во всех процессах и планируемыми для выполнения аппаратными потоками. Обычно можно создать программных потоков больше, чем аппаратных, поскольку, когда программный поток заблокирован (например, при вводе-выводе или ожидании мьютекса или переменной условия), пропускная способность может быть повышена путем выполнения других, незаблокированных потоков.
> 
> **std::thread** представляют собой объекты в процессе С++, которые действуют как дескрипторы для лежащих в их основе программных потоков. Некоторые объекты [[thread#std::thread|std::thread]] представляют "нулевые" дескрипторы, т.е. не соответствуют программным потокам, поскольку находятся в состоянии, сконструированном по умолчанию (следовательно, без выполняемой функции); потоки, из которых выполнено перемещение (после перемещения объект [[thread#std::thread|std::thread]], в который оно произошло, действует как дескриптор для соответствующего программного потока); потоки, у которых выполняемая ими функция завершена; а также потоки, у которых разорвана связь между ними и обслуживающими их программными потоками.

Программные потоки являются ограниченным ресурсом. Если вы попытаетесь создать их больше, чем может предоставить система, будет сгенерировано исключение [[system_error|std::system_error]]. Это так, даже если функция, которую вы хотите запустить, не генерирует исключений. Например, даже если `doAsyncwork` объявлена как [[noexcept|noexcept]],

```c++
int doAsyncWork ( ) noexoept;
```

следующая инструкция может сгенерировать исключение:

```c++
std::thread t(doAsyncWork);     // Генерация исключения, если
								// больше нет доступных потоков
```

Хорошо написанное программное обеспечение должно каким-то образом обрабатывать такую возможность, но как? Один вариант - запустить `doAsyncWork` в текущем потоке, но это может привести к несбалансированной нагрузке и, если текущий поток является потоком GUI, к повышенному времени реакции системы на действия оператора. Другой вариант - ожидание завершения некоторых существующих программных потоков с последующей попыткой создания нового объекта [[thread#std::thread|std::thread]], но может быть и так, что существующие потоки ожидают действий, которые должна выполнить функция `doAsyncWork` (например, ее результата или уведомления переменной условия).

Даже если вы не исчерпали потоки, у вас могут быть проблемы с превышением подписки (`oversubscription`). Это происходит, когда имеется больше готовых к запуску (т.е. незаблокированных) программных потоков, чем аппаратных. Когда это случается, планировщик потоков (который обычно представляет собой часть операционной системы) выполняет разделение времени для выполнения программных потоков аппаратными. Когда время работы одного потока завершается и начинается время работы второго, выполняется переключение контекстов. Такие переключения контекстов увеличивают накладные расходы по управлению потоками и могут оказаться в особенности дорогостоящими, когда аппаратный поток, назначаемый программному, оказывается выполняемым другим ядром, не тем, что ранее. В этом случае ( 1 ) кеши процессора обычно оказываются с данными, не имеющими отношения к данному программному потоку, и (2) запуск "нового" программного потока на этом ядре "загрязняет" кеши процессора, заполняя их данными, не имеющими отношения к "старым" потокам, которые выполнялись этим ядром и, вероятно, будут выполняться им снова.

Избежать превышения подписки сложно, поскольку оптимальное отношение программных и аппаратных потоков зависит от того, как часто запускаются программные потоки, и может изменяться динамически, например, при переходе программы от работы по вводу-выводу к вычислениям. Наилучшее отношение программных и аппаратных потоков зависит также от стоимости переключения контекстов и от того, насколько эффективно программные потоки используют кеши процессора. Кроме того, количество аппаратных потоков и подробная информация о кешах процессора (например, насколько они велики и какова их относительная скорость) зависят от архитектуры компьютера, так что, даже если вы настроите свое приложение так, чтобы избежать превышения подписки (сохраняя занятость аппаратного обеспечения) на одной платформе, нет никакой гарантии, что ваше решение будет хорошо работать и на других видах машин.

Ваша жизнь станет легче, если вы переложите свои проблемы на чужие плечи, и это плечо вам готов подставить [[async|std::async]]:

```c++
auto fut = std::async(doAsyncWork);     // Управление потоками лежит
                                        // на реализации стандартной
                                        // библиотеки
```

Этот вызов переносит ответственность за управление потоками на реализацию стандартной библиотеки С++. Например, вероятность получения исключения нехватки потоков значительно снижается, поскольку этот вызов, вероятно, никогда его не сгенерирует. "Как такое может быть? - удивитесь вы. - Если я запрошу больше потоков, чем может предоставить система, то какая разница, как это будет сделано - через создание [[thread#std::thread|std::thread]] или вызовом [[async|std::async]]? " Это имеет значение, поскольку [[async|std::async]], будучи вызвана в данном виде (т.е. со стратегией запуска по умолчанию), не гарантирует [[Parallel computing#Если важна асинхронность, указывайте launch_async std launch async|создания нового программного потока]]. Она скорее разрешает планировщику организовать для указанной функции (в нашем примере - `doAsyncWork`) возможность запуска потоком, запрашивающим результат `doAsyncWork` (например, в потоке, вызывающем `get` или `wait` для объекта `fut`), и интеллектуальные планировщики воспользуются предоставленной свободой, если в системе превышена подписка или не хватает потоков.

Если вы попытаетесь проделать этот "запуск в потоке, запрашивающем результат" самостоятельно, то я замечу, что это может привести к вопросам о дисбалансе нагрузки, и эти вопросы не исчезнут просто потому, что [[async|std::async]] и планировщик времени выполнения возьмутся за дело вместо вас. Тем не менее, когда дело доходит до балансировки нагрузки, планировщик времени выполнения, скорее всего, будет иметь более полную картину происходящего на машине, чем вы, потому что он управляет потоками всех процессов, а не только вашего.

При применении [[async|std::async]] время отклика потока GUI может остаться проблематичным, поскольку планировщику неизвестно, какой из ваших потоков имеет повышенные требования к этому параметру. В таком случае вы можете захотеть передать в [[async|std::async]] стратегию запуска `std::launch::async`. Это гарантирует, что интересующая вас функция будет действительно выполнена другим потоком.

Современные планировщики потоков во избежание превышения подписки используют пулы потоков всей системы и повышают балансировку загрузки между аппаратными ядрами с помощью специальных алгоритмов. Стандарт С++ не требует применения пулов потоков или конкретных алгоритмов, и, честно говоря, есть некоторые технические аспекты спецификации параллельности в С++11, которые делают его применение более трудным, чем хотелось бы. Тем не менее некоторые производители используют преимущества указанных методов в своих реализациях стандартной библиотеки, и следует ожидать продолжения прогресса в данной области. Если вы примете для параллельного программирования подход на основе задач, вы будет автоматически пользоваться ее преимуществами, которые будут возрастать с ее распространенностью. С другой стороны, программируя непосредственно с помощью [[thread#std::thread|std::thread]], вы берете на себя бремя борьбы с исчерпанием потоков, превышением подписки и балансировкой загрузки (не упоминая уже о том, насколько ваши решения этих проблем будут совместимы с решениями в программах, работающих в других процессах на том же компьютере).

По сравнению с программированием на основе потоков программирование на основе задач спасает вас от управления потоками вручную и обеспечивает естественный способ получения результатов асинхронно выполненных функций (т.е. возвращаемых значений или исключений). Тем не менее существуют ситуации, в которых применение потоков может быть более подходящим. Они включают в себя следующее.

> Вам нужен доступ к API, лежащей в основе реализации потоков. В С++ API параллельных вычислений обычно реализуется с помощью низкоуровневого платформозависимого API, обычно `pthreads` или `Windows Threads`. Эти API в настоящее время предлагают больше возможностей, чем С++. (Например, С++ не имеет понятий приоритетов или родственности потоков.) Для предоставления доступа к API реализации потоков [[thread#std::thread|std::thread]] обычно предлагает функцию-член [[native_handle|native_handle]]. Такая функциональность отсутствует в [[future|std::future]] (т.е. в том, что возвращает [[async|std::async]]).
> 
> Вам требуется возможность оптимизации потоков в вашем приложении. Это может произойти, например, если вы разрабатываете серверное программное обеспечение с известным профилем выполнения, которое может быть развернуто как единственный процесс на машине с фиксированными аппаратными характеристиками.
> 
> Вам требуется реализовать поточную технологию, выходящую за рамки API параллельных вычислений в С++, например пулы потоков на платформах, на которых ваши реализации С++ их не предоставляют.

## Если важна асинхронность, указывайте `std::launch::async`

Вызывая [[async|std::async]] для выполнения функции (или иного выполнимого объекта), вы в общем случае планируете выполнять ее асинхронно. Но вы не обязательно требуете это от [[async|std::async]]. В действительности вы запрашиваете выполнение функции в соответствии со стратегией запуска [[async|std::async]]. Имеются две стандартные стратегии, представленные перечислителями в перечислении с областью видимости `std::launch`. ([[Предпочитайте перечисления с областью видимости перечислениям без таковой|информация о перечислениях с областью видимости]]) В предположении, что для выполнения в [[async|std::async]] передается функция `f`:

* стратегия `std::launch::async` означает, что `f` должна выполняться асинхронно, т.е. в другом потоке;
* стратегия `std::launch::deferred` означает, что ё может выполняться только тогда, когда для фьючерса, возвращенного [[async|std::async]], вызывается функция-член `get` или `wait`, т.е. выполнение `f` откладывается до тех пор, пока не будет выполнен такой вызов. Когда вызываются функции-члены `get` или `wait`, функция `f` выполняется синхронно, т.е. вызывающая функция блокируется до тех пор, пока `f` не завершит работу. Если не вызывается ни `get`, ни `wait`, `f` не выполняется.

Возможно, это окажется удивительным, но стратегия запуска [[async|std::async]] по умолчанию - используемая в случае, если вы не указали таковую - не является ни одной из перечисленных. На самом деле это стратегия, которая представляет собой сочетание описанных с помощью оператора "или': Два приведенных далее вызова имеют один и тот же смысл:

```c++
auto fut1 = std::async(f);        // Выполнение f со стратегией
								  // запуска по умолчанию

auto fut2 = std::async(           // Выполнение f
		std::launch::async |      // асинхронное
		std::launch::deferred,    // или отложенное
		f
);
```

Таким образом, стратегия по умолчанию позволяет `f` быть выполненной как асинхронно, так и синхронно. Как указано в [[Parallel computing#Предпочитайте программирование на основе задач программированию на основе потоков|разделе]] , эта гибкость позволяет [[async|std::async]] и компонентам управления потоками стандартной библиотеки брать на себя ответственность за создание и уничтожение потоков, предупреждение превышения подписки и баланс загрузки. Все это делает параллельное программирование с помощью [[async|std::async]] таким удобным.

Но применение [[async|std::async]] со стратегией запуска по умолчанию имеет некоторые интересные последствия. Для потока `t`, выполняющего приведенную ниже инструкцию, справедливы следующие утверждения.

```c++
auto fut = std::async(f);   // Вьполнение f со стратегией
							// запуска по умолчанию
```

> **Невозможно предсказать, будет ли `f` выполняться параллельно с `t`**, поскольку выполнение `f` может быть отложено планировщиком.
> 
> **Невозможно предсказать, будет ли `f` выполняться потоком, отличным от тоrо, в котором вызываются функции-члены `get` или `wait` объекта `fut`**. Если этот поток - `t`, отсюда вытекает невозможность предсказать, будет ли `f` выполняться потоком, отличным от `t`.
> 
> **Может быть невозможно предсказать, будет ли `f` выполнена вообще**, поскольку может оказаться невозможно гарантировать, что функции-члены `get` или `wait` объекта `fut` будут вызваны на всех путях выполнения программы.

Гибкость планирования стратегии запуска по умолчанию часто плохо комбинируется с использованием переменных [[Parallel programming#Базовые потоки типа std thread|thread_local]], поскольку она означает, что если `f` читает или записывает такую локальную память потока (**thread-local storage** - `TLS`), то невозможно предсказать, к переменным какого потока будет обращение:

```c++
auto fut = std::async(f);       // TLS для f может принадлежать
								// независимому потоку, но может
								// принадлежать и потоку, вызывающему
								// get или wait объекта fut
```

Это также влияет на циклы на основе `wait` с использованием тайм-аутов, поскольку вызов `wait_for` или `wait_until` для откладываемой задачи (см. [[Parallel computing#Предпочитайте программирование на основе задач программированию на основе потоков|раздел]]) дает значение `std::future_status::deferred`. Это означает, что приведенный далее цикл, который выглядит как в конечном итоге завершающийся, может оказаться бесконечным:

```c++
using namespace std::literals;   // Суффиксы длительности С++14;

void f()                         // f ждет 1 с, затем выполняется
{                                // возврат из функции
	std::this_thread::sleep_for(1s);
}

auto fut = std::async(f);        // (Концептуально) асинхронное
								 // выполнение функции f
while(fut.wait_for(100ms) ! =    // Цикл до завершения f . . .
	std::future_status::ready)
{                                // . . . которого никогда не будет !
}
```

Если функция `f` выполняется параллельно с потоком, вызывающим [[async|std::async]] (т.е. если выбранная для `f` стратегия запуска - `std::launch::async`) , нет никаких проблем (в предположении, что `f` в конечном итоге завершится), но если выполнение `f` откладывается, `fut.wait_for` всегда будет возвращать `std::future_status::deferred`. Это возвращаемое значение никогда не станет равным `std::future_status::ready`, так что цикл никогда не завершится.

Такого рода ошибки легко упустить во время разработки и модульного тестирования, потому что они могут проявляться только при больших нагрузках. При этом возможно превышение подписки или исчерпание потоков, и в такой ситуации задача, скорее всего, будет отложена. В конечном итоге, если аппаратному обеспечению не угрожает превышение подписки или исчерпание потоков, у системы времени выполнения нет никаких оснований для того, чтобы не запланировать параллельное выполнение задачи.

Исправить ошибку просто: следует проверить фьючерс, соответствующий вызову [[async|std::async]], и выяснить, не отложена ли данная задача. Если отложена, то надо избегать входа в цикл на основе таймаутов. К сожалению, нет непосредственного способа выяснить у фьючерса, отложена ли задача. Вместо этого вы должны вызывать функцию на основе таймаута, такую как `wait_for`. В этом случае вы в действительности не хотите ничего ожидать, а хотите просто проверить, не возвращает ли она значение `std::future_status::deferred`, так что можно вызывать эту функцию с нулевым временем ожидания:

```c++
auto fut = std::async(f);               // Как и ранее

if (fut.wait_for(Os) ==                 // Если задача отложена...
	std::future_status::deferred)
{
								        // ...используем wait или get
	...							        // для синхронного вызова f
} else {						        // Если задача не отложена -
	while(fut.wait_for(100ms) !=        // бесконечный цикл
		std::future_status::ready) {    // невозможен ( если f в
										// конце концов завершается)
										// Задача не отложена и не готова,
									// так что она выполняется параллельно
		}
										// fut выполнен
}
```

Из всего изложенного получается, что применение [[async|std::аsуnс]] со стратегией запуска по умолчанию отлично работает, пока выполняются следующие условия.

* Задача не обязана работать параллельно с потоком, вызывающим `get` или `wait`.
* Не имеет значения, переменные `thread_local` какого потока читаются или записываются.
* Либо гарантируется вызов `get` или `wait` для фьючерса, возвращаемого [[async|std::async]], либо ситуация, когда задача не выполняется совсем, является приемлемой.
* Код с использованием `wait_for` или `wait_uпtil` учитывает возможность отложенного состояния задачи.

Если не выполняется любое из этих условий, то вы, вероятно, захотите гарантировать, что [[async|std::async]] обеспечит асинхронное выполнение задачи. Для этого в качестве первого аргумента в нее надо передать значение `std::launch::async`:

```c++
auto fut = std::async(std::launch::async, f); // Асинхронный
											  // запуск f
```

На практике удобно иметь функцию, работающую как [[async|std::async]], но автоматически использующую стратегию запуска `std::launch::async`. Такую функцию несложно написать. Вот как выглядит версия этой функции для С++11:

```c++
template<typename F, typename... Ts>
inline
std::future<typename std::result_of<F(Ts...)>::type>
reallyAsync(F&& f, Ts&&... params)           // Возврат фьючерса
{                                            // для асинхронного
	return std::async(std::launch::async,    // вызова f (params...)
					std::forward<F>(f),
					std::forward<Ts>(params)...);
}
```

Эта функция получает вызываемый объект `f` и нуль или более параметров `params` и выполняет их [[Использование move и forward|прямую передачу]] в [[async|std::async]], передавая также значение `std::launch::async` в качестве стратегии вызова. Подобно [[async|std::async]], она возвращает [[future|std::future]] для результата выполнения `f` с параметрами `params`. Определить тип этого результата просто, так как его нам дает свойство типа [[result_of|std::result_of]]. 

Функция `reallyAsync` используется так же, как и [[async|std::async]]:

```c++
auto fut = reallyAsync(f);    // Асинхронный запуск f; генерирует
                              // исключение, если это делает
							  // std::аsупс
```

В С++14 возможность вывода возвращаемого типа `reallyAsync` сокращает объявление функции:

```c++
template<typename F, typename... Ts>
inline
auto                                                   // С++14
reallyAsync(F&& f, Ts&&... params)
{
	return std::async(std::launch::async,
					std::forward<F>(f),
					std::forward<Ts>(params)...);
}
```

Эта версия делает кристально ясным то, что `reallyAsync` не делает ничего, кроме вызова [[async|std::async]] со стратегией запуска `std::launch::async`.

* Стратегия запуска по умолчанию для [[async|std::async]] допускает как асинхронное, так и синхронное выполнение задачи.
* Эта гибкость ведет к неопределенности при обращении к переменным `thread_local`, к тому, что задача может никогда не быть выполнена, и влияет на логику программы для вызовов `wait` на основе тайм-аутов.
* Если асинхронное выполнение задачи критично, указывайте стратегию запуска std::launch::async.

## Делайте std::thread неподключаемым на всех путях выполнения

Каждый объект [[thread#std::thread|std::thread]] может находиться в двух состояниях: подключаемом (joinable) и неподключаемом (unjoinable). Подключаемый [[thread#std::thread|std::thread]] соответствует асинхронному потоку выполнения, который выполняется или может выполняться. Например, таковым является [[thread#std::thread|std::thread]], соответствующий потоку, который заблокирован или ожидает запуска планировщиком. Объекты [[thread#std::thread|std::thread]], соответствующие потокам, которые выполняются до полного завершения, также рассматриваются как подключаемые.

Неподключаемые объекты [[thread#std::thread|std::thread]] являются именно тем, что вы и ожидаете, а именно - объектами [[thread#std::thread|std::thread]], которые не являются подключаемыми. Неподключаемые объекты [[thread#std::thread|std::thread]] включают следующие.

* **Объекты [[thread#std::thread|std::thread]], созданные конструкторами по умолчанию**. Такие [[thread#std::thread|std::thread]] не имеют выполняемой функции, а значит, не соответствуют никакому потоку выполнения.
* **Объекты [[thread#std::thread|std::thread]], из которых выполнено перемещение**. В результате перемещения поток выполнения, соответствующий [[thread#std::thread|std::thread]], становится соответствующим другому объекту [[thread#std::thread|std::thread]].
* **Объекты [[thread#std::thread|std::thread]], для которых выполнена функция-член `join`**. После выполнения функции-члена `join` объект [[thread#std::thread|std::thread]] больше не соответствует потоку выполнения, который при этом вызове полностью завершается.
* **Объекты [[thread#std::thread|std::thread]], для которых выполнена функция-член `detach`**. Функция член `detach` разрывает связь между объектом [[thread#std::thread|std::thread]] и соответствующим ему потоком выполнения.

Одной из причин, по которым так важна подключаемость [[thread#std::thread|std::thread]], является то, что при вызове деструктора для подключаемого объекта программа завершает свою работу. Предположим, например, что у нас есть функция `doWork`, которая получает функцию фильтрации `filter` и максимальное значение `maxVal` в качестве параметров. Функция `doWork` выполняет проверку, чтобы убедиться, что все условия, необходимые для ее работы, выполнены, а затем выполняет вычисления со всеми значениями от `0` до `maxVal`, проходящими через фильтр. Если фильтрация и определение выполнения условий для функции `doWork` требуют большого времени выполнения, может быть разумным выполнить эти два действия параллельно.

Мы бы предпочли программировать параллельность на основе задач, но предположим, что нам надо установить приоритет потока, выполняющего фильтрацию. Для этого требуется системный дескриптор потока, а он доступен только через API [[thread#std::thread|std::thread]]; API задач (т.е. фьючерсы) такой возможности не предоставляет. Поэтому мы работаем с потоками, а не с задачами.

Мы могли бы начать с кода наподобие следующего:

```c++
constexpr auto tenМillion = 10 000 000;

// Возвращает значение, указывающее, были ли выполнены
// вычисления; std::function
bool doWork(std::function<bool(int)> filter,
				int maxVal = tenМillion)
{
	// Значения, удовлетворяющие фильтру :
	std::vector<int> goodVals;
	
	// Заполнение goodVals:
	std::thread t([&filter, maxVal, &goodVals]
			{
				for(auto i=О; i <= maxVal; ++i)
				{ 
					if(filter(i)) 
						goodVals.push_back(i);
				}
			});
			
	// Используем системный дескриптор потока для
	// установки приоритета:
	auto nh = t.native_handle();
	
	if(conditionsAreSatisfied()) {
		t.join();                        // Завершаем t
		performComputation(goodVals);
		return true;                     // Вычисление вьшолено
	}
	
	return false;                        // Вычисление не вьmолено
}
```

Перед тем как пояснить, почему этот код проблематичен, я замечу, что инициализирующее значение `tenMillion` в С++14 можно сделать более удобочитаемым, воспользовавшись возможностью C++14 использовать апостроф для разделения цифр:

```c++
constexpr auto tenМillion = 10'000'000;         // С++14
```

Замечу также, что установка приоритета `t` после его запуска напоминает забивание двери конюшни после того, как лошадь уже убежала. Лучше начинать с `t` в приостановленном состоянии (тем самым делая возможным изменение его приоритета до выполнения вычислений}, но я не хочу отвлекать вас этим кодом. 

Вернемся к функции `doWork`. Если вызов `conditionsAreSatisfied()` возвращает
`true`, все в порядке, но если он вернет значение `false` или сгенерирует исключение, объект `t` будет подключаемым в момент вызова деструктора при окончании работы `doWork`. Это приведет к завершению работы программы.

Вы можете удивиться, почему деструктор [[thread#std::thread|std::thread]] ведет себя столь неподобающе. Да просто потому, что два других варианта, описанных далее, еще хуже.

> - **Неявный вызов `join`**. В этом случае деструктор [[thread#std::thread|std::thread]] будет ожидать завершения соответствующего асинхронного потока. Звучит разумно, но может привести к аномалиям производительности, которые будет трудно отследить. Например, было бы нелогичным, чтобы функция `doWork` ожидала применения фильтра ко всем значениям, если вызов `conditionsAreSatisfied()` уже вернул значение `false`.
> 
> - **Неявный вызов `detach`**. В этом случае деструктор [[thread#std::thread|std::thread]] разрывает связь между объектом [[thread#std::thread|std::thread]] и его потоком, отключая последний от объекта. Поток продолжает выполняться. Это звучит не менее разумно, чем применение `join`, но проблемы отладки, к которым это может привести, делают этот вариант еще худшим. Например, в функции `doWork` локальная переменная `goodVals` захватывается лямбда-выражением по ссылке. Она также изменяется в лямбда-выражении (с помощью вызова `push_back`) . Предположим теперь, что во время асинхронного выполнения лямбда-выражения вызов `conditionsAreSatisfied()` вернул `false`. В таком случае функция `doWork` должна завершиться, а ее локальные переменные (включая `goodVals`) должны быть уничтожены. Ее кадр стека удаляется, и выполнение потока продолжается с точки вызова `doWork`.
> - Инструкции после этой точки могут в некоторой иной точке выполнять вызовы других функций, и как минимум одна из них может занять место в стеке, ранее занятое кадром стека `doWork`. Назовем эту функцию `f`. Во время работы `f` лямбда-выражение из `doWork` продолжает асинхронно выполняться и может вызвать `push_back` для памяти в стеке, которая ранее использовалась для локальной переменной `goodVals`, а теперь принадлежит кадру стека `f`. Такой вызов может изменить память, использовавшуюся для `goodVals`, а это означает, что с точки зрения `f` содержимое памяти в ее кадре стека может внезапно измениться! Только представьте себе, что вам придется отлаживать такое.

Комитет по стандартизации решил, что последствия уничтожения подключаемого потока достаточно неприятны, чтобы полностью их запретить (указав, что уничтожение подключаемого потока приводит к завершению программы).

Это решение возлагает на вас ответственность за то, что если вы используете объект [[thread#std::thread|std::thread]], то должны сделать его неподключаемым на всех путях из области видимости, в которой он определен. Но охват всех путей может быть весьма сложным. Он включает как нормальный выход из области видимости, так и такие выходы, как с помощью `return`, `continue`, `break`, `goto` или исключения. Этих путей может быть великое множество.

Всякий раз, когда надо выполнить некоторое действие на всех путях, ведущих из блока, естественным подходом является размещение этого действия в деструкторе локального объекта. Такие объекты известны как объекты `RAII`, а соответствующие классы - классы `RAII`. (RAII означает "Resource Acquisition Is Initialization': "захват ресурса есть инициализация": хотя на самом деле речь идет о методе деструкции, а не инициализации.) RAII-классы - распространенное явление в стандартной библиотеке. Примерами являются контейнеры STL (деструктор каждого контейнера уничтожает его содержимое и освобождает память), стандартные интеллектуальные указатели (деструктор [[unique_ptr|std::unique_ptr]] вызывает удалитель для объекта, на который указывает, а деструкторы [[shared_ptr|std::shared_ptr]] и [[weak_ptr|std::weak_ptr]] уменьшают счетчики ссылок), объекты [[fstream|std::fstream]] (их деструкторы закрывают соответствующие файлы) и многое другое. Тем не менее стандартного RAll-клacca для объектов [[thread#std::thread|std::thread]] нет, вероятно, потому что Комитет по стандартизации, отвергнув и `join`, и `detach` как варианты по умолчанию, просто не знал, что же должен делать такой класс.

К счастью, такой класс несложно написать самостоятельно. Например, приведенный далее класс позволяет вызывающей функции указать, должна ли быть вызвана функция-член `join` или `detach` при уничтожении объекта `ThreadRAII` (объект RAII для [[thread#std::thread|std::thread]]):

```c++
class ThreadRAII {
	public:
		enum class DtorAction {join, detach};

		ThreadRAII(std::thread&& t, DtorAction а) // Деструктор для
		:action(a), t(std::move(t)) { }           // t вьmолняет а

		~ThreadRAII()
		{
			if (t.joinable()) {
				if(action == DtorAction::join) {
					t.join();				
				}			
				else{
					t.detach();				
				}
			}		
		}
		
		std::thread& get() {return t;}	
	private:
		DtorAction action;
		std::thread t;
};
```
Я надеюсь, что этот код самодостаточен и не требует особых пояснений, тем не менее может быть полезно остановиться на следующих моментах.

> Конструктор принимает только [[rvalue|rvalue]] [[thread#std::thread|std::thread]], поскольку мы хотим перемещать передаваемый объект [[thread#std::thread|std::thread]] в объект `ThreadRAII`. (Вспомните, что объекты [[thread#std::thread|std::thread]] не копируются.)
> 
> Порядок параметров выбран интуитивно понятным для программиста (сначала указывается поток, а затем - способ его деструкции), но список инициализации членов соответствует порядку объявлений членов-данных. Этот порядок размещает объект [[thread#std::thread|std::thread]] последним. В этом классе порядок не имеет значения, но в общем случае возможна ситуация, когда инициализация одного члена-данных зависит от другого, а поскольку объекты [[thread#std::thread|std::thread]] могут запускаться на выполнение немедленно после их инициализации, объявлять их последними в классе - неплохая привычка. Это гарантирует, что в момент их создания все члены-данные, им предшествующие, уже инициализированы, и, таким образом, асинхронно выполняемый поток, соответствующий объекту [[thread#std::thread|std::thread]], может безопасно к ним обращаться.
> 
> `ThreadRAII` предоставляет функцию `get`, обеспечивающую доступ к соответствующему объекту [[thread#std::thread|std::thread]]. Это аналог функций `get`, предоставляемых стандартными интеллектуальными указателями и обеспечивающих доступ к их базовым обычным указателям. Предоставление `get` позволяет избежать необходимости дублировать в классе `ThreadRAII` весь интерфейс [[thread#std::thread|std::thread]], а также означает, что объекты `ThreadRAII` могут использоваться в контекстах, где требуются объекты [[thread#std::thread|std::thread]].
> 
> Перед тем как деструктор `ThreadRAII` вызовет функцию-член объекта `t` типа [[thread#std::thread|std::thread]], он проверяет, является ли этот объект подключаемым. Это необходимо, поскольку применение `join` или `detach` к неподключаемому объекту приводит к неопределенному поведению. Может быть так, что клиент создает [[thread#std::thread|std::thread]], затем создает из неrо объект `ThreadRAII`, использует функцию-член `get` для получения доступа к `t`, а затем выполняет перемещение из `t` или вызывает для него `join` или `detach`. Каждое из этих действий делает `t` неподключаемым.

```c++
if(t.joinable()) {
	if (action == DtorAction::join) {
		t.join();
	}
	else {
		t.detach();
	}
}
```

Если в приведенном фрагменте вас беспокоит возможность условия гонки из-за того, что между вызовами `t.joinable()` и `join` или `detach` другой поток может сделать `t` неподключаемым, то ваша интуиция заслуживает похвалы, но ваши опасения в данном случае беспочвенны. Объект [[thread#std::thread|std::thread]] может изменить состояние с подключаемого на неподключаемое только путем вызова функции-члена, например `join`, `detach` или операции перемещения. В момент вызова деструктора `ThreadRAII` никакие другие потоки не должны вызывать функцию-член для этого объекта. При наnичии одновременных вызовов, определенно, имеется условие гонки, но не внутри деструктора, а в клиентском коде, который пытается вызвать одновременно две функции-члена объекта (деструктор и что-то еще). В общем случае одновременные вызовы функций-членов для одного объекта безопасны, только если все они являются [[Делайте константные функции-члены безопасными в смысле потоков|константными функциями-членами]].

Использование `ThreadRAII` в нашей функции `doWork` может выглядеть следующим образом:

```c++
bool doWork (std::function<bool(int)> filter, // Как и ранее
					int maxVal = tenМillion)
{
	std::vector<int> goodVals;                // Как и ранее
	ТhreadRAII t(                             // use RAII object
		std::thread([&filter, maxVal, &goodVals]
			{
				for(auto i = О ; i <= maxVal; ++i)
				{ 
					if(filter(i)) 
						goodVals.push_back(i);
				}
			}),
		ТhreadRAII::DtorAction::join          // Действие RAII
	);

	auto nh = t.get().native_handle();
	...
	if(conditionsAreSatisfied()) {
		t.get().join();
		performComputation(goodVals);
		return true;
	}
	return false;
}
```

В этом случае мы выбрали использование `join` для асинхронно выполняющегося потока в деструкторе `ThreadRAII`, поскольку, как мы видели ранее, применение `detach` может привести к настоящим кошмарам при отладке. Мы также видели ранее, что применение `join` может вести к аномалиям производительности (что, откровенно говоря, также может быть неприятно при отладке), но выбор между неопределенным поведением (к которому ведет `detach`), завершением программы (при использовании обычного [[thread#std::thread|std::thread]]) и аномалиями производительности предопределен - мы выбираем меньшее из зол.

Увы, применение `ThreadRAII` для выполнения `join` при уничтожении [[thread#std::thread|std::thread]] иногда может привести не к аномалии производительности, а к полному "зависанию" программы. "Правильным" решением этого типа проблем было бы сообщить асинхронно выполняющемуся лямбда-выражению, что в ero услугах мы больше не нуждаемся и оно должно поскорее завершиться. Увы, С++11 не поддерживает прерываемые потоки. Их можно реализовать вручную.

В [[Генерация специальных функций-членов|разделе]] поясняется, что, поскольку `ThreadRAII` объявляет деструктор, в нем нет генерируемых компилятором перемещающих операций, но причин, по которым `ThreadRAII` не должен быть перемещаемым, тоже нет. Если бы компилятор генерировал такие функции, то они демонстрировали бы верное поведение, так что просто попросим компилятор их все же сгенерировать:

```c++
class ThreadRAII {
	public:
		enum class DtorAction {join, detach};     // Как и ранее

		ThreadRAII(std::thread&& t, DtorAction а) // Как и ранее
			:action(а), t(std::move(t)) {}

		~ThreadRAII()
		{
												  // Как и ранее
		}

		ТhreadRAII(ТhreadRAII&&) = default;               // Поддержка
		ТhreadRAII& operator= (ТhreadRAII&&) = default;   // перемещения
		
		std::thread& get() { return t; }                  // Как и ранее

	private:
		DtorActioп action;
		std::thread t;
);
```

> Делайте [[thread#std::thread|std::thread]] неподключаемыми на всех путях выполнения.
> 
> Применение `join` при уничтожении объекта может привести к трудно отлаживаемым аномалиям производительности.
> 
> Применение `detach` при уничтожении объекта может привести к трудно отлаживаемому неопределенному поведению.
> 
> Объявляйте объекты [[thread#std::thread|std::thread]] в списке членов-данных последними.

## Помните о разном поведении деструкторов дескрипторов потоков

В предыдущем разделе узнали, что подключаемый [[thread#std::thread|std::thread]] соответствует базовому системному потоку выполнения. [[Parallel computing#Если важна асинхронность, указывайте launch_async std launch async|Фьючерс для неотложенной]] задачи  имеет схожую связь с системным потоком. А раз так, и объекты [[thread#std::thread|std::thread]], и объекты фьючерсов можно рассматривать как дескрипторы (`handles`) системных потоков.

С этой точки зрения интересно, что [[thread#std::thread|std::thread]] и фьючерсы совершенно по-разному ведут себя в деструкторах. Как упоминалось в [[Parallel computing#Делайте std thread неподключаемым на всех путях выполнения|разделе]], уничтожение подключаемого [[thread#std::thread|std::thread]] завершает работу программы, поскольку две очевидные альтернативы - неявный вызов `join` и неявный вызов `detach` оказываются еще более плохим выбором. Однако деструктор фьючерса ведет себя так, как если бы иногда выполнялся не явный вызов `join`, иногда - неявный вызов `detach`, а иногда - ни то и ни другое. Он никогда не приводит к завершению работы программы. Поведение этого дескриптора потока заслуживает более внимательного рассмотрения.

Начнем с наблюдения, что фьючерс представляет собой один из концов канала связи, по которому вызываемая функция передает результаты вызывающей. Вызываемая функция (обычно работающая асинхронно) записывает результат вычислений в коммуникационный канал (обычно с помощью объекта [[promise|std::promise]]), а вызывающая функция читает результат с помощью фьючерса. Вы можете представлять это для себя следующим образом (пунктирные стрелки показывают поток информации от вызываемой функции к вызывающей):

![[async.png]]

Но где же хранится результат вызываемой функции? Вызываемая функция может завершиться до того, как будет вызвана функция-член `get` соответствующего фьючерса, так что результат не может быть сохранен в [[promise|std::promise]] вызываемой функции. Этот объект, будучи локальным по отношению к вызываемой функции, уничтожается по ее завершении.

Результат не может храниться и во фьючерсе вызывающей функции, поскольку
(среди прочих причин) [[future#std future|std::future]] может быть использован для создания объекта [[future#std shared_future|std::shared_future]] (тем самым передавая владение результатом вызываемой функции от [[future#std future|std::future]] в [[future#std shared_future|std::shared_future]]), который затем, после уничтожения исходного [[future#std future|std::future]], может быть многократно копирован. С учетом того, что не все типы результата могут быть скопированы (например, существуют только перемещаемые типы) и что результат должен существовать до тех пор, пока как минимум последний фьючерс на него ссылается, какой из потенциально многих фьючерсов, соответствующих вызываемой функции, должен содержать ее результат?

Поскольку ни объекты, связанные с вызываемой функцией, ни объекты, связанные с вызывающей функцией, не являются подходящими местами для хранения результата вызываемой функции, они должны храниться где-то вне этих объектов. Такое местоположение известно как общее состояние (shared state). Это общее состояние обычно представлено объектом в динамической памяти, но его тип, интерфейс и реализация в стандарте языка не указаны. Авторы стандартной библиотеки могут реализовывать общие СОСТОЯНИЯ так, как хотят.

Мы можем представить себе отношения между вызываемой функцией, вызывающей функцией и общим состоянием следующим образом (пунктирными стрелками вновь представлен поток информации):

![[async_2.png]]

Существование общего состояния имеет важное значение, потому что поведение деструктора фьючерса - тема этого раздела - определяется общим состоянием, связанным с этим фьючерсом. В частности, справедливо следующее.

> - **Деструктор последнего фьючерса, ссылающегося на общее состояние для неотложенной задачи, запущенной с помощью [[async|std::async]]**, блокируется до тех пор, пока задача не будет завершена. По сути, деструктор такого фьючерса неявно выполняет `join` для потока, асинхронно выполняющего задачу.
> 
> - **Деструкторы всех прочих фьючерсов просто уничтожают объект фьючерса**. Для асинхронно выполняющихся задач это сродни неявному отключению базового потока с помощью вызова `detach`. Для отложенных задач, для которых данный фьючерс является последним, это означает, что отложенная задача никогда не будет выполнена.

Эти правила выглядят сложнее, чем есть на самом деле. Мы имеем дело с простым "нормальным" поведением и одним исключением. Нормальное поведение заключается в том, что деструктор фьючерса уничтожает объект фьючерса. Вот и все. Он ничего не подключает, ничего не отключает, он ничего не запускает. Он просто уничтожает члены-данные фьючерса. (Ну, на самом деле он делает еще одно дело - уменьшает значение счетчика ссылок общего состояния, которое управляется как ссылающимися на него фьючерсами, так и объектами [[promise|std::promise]] вызываемой функции. Этот счетчик ссылок позволяет библиотеке знать, когда можно уничтожать общее состояние. [[shared_ptr|Информация по счетчикам ссылок]].

Исключение из этого нормального поведения осуществляется только для фьючерса, для которого выполняются все перечисленные далее условия.

> Он ссылается на общее состояние, созданное вызовом [[async|std::async]].
> 
> Стратегия запуска задачи - [[Parallel computing#Если важна асинхронность, указывайте launch_async std launch async|std::launch::async]], либо потому, что она выбрана системой времени выполнения, либо потому, что была явно указана в вызове [[async|std::async]].
> 
> Фьючерс является последним фьючерсом, ссылающимся на общее состояние. Это всегда справедливо для [[future#std future|std::future]]. Для [[future#std shared_future|std::shared_future]], если при уничтожении фьючерса на то же самое общее состояние ссылаются другие [[future#std shared_future|std::shared_future]], поведение уничтожаемого фьючерса - нормальное (т.е. просто уничтожаются его члены-данные).

Только когда все эти условия выполнены, деструктор фьючерса демонстрирует особое поведение, и это поведение состоит в блокировке до тех пор, пока не будет завершена асинхронно выполняющаяся задача. С практической точки зрения это равносильно неявному вызову `join` для потока с запущенной задачей, созданной с помощью [[async|std::async]].

Часто приходится слышать, что это исключение из нормального поведения деструктора фьючерса резюмируется как "Фьючерс из [[async|std::async]] блокируется в своем деструкторе". В качестве первого приближения это так, но иногда нам надо что-то большее, чем первое приближение. Теперь вы знаете правду во всей ее красе и славе. 

Ваше удивление может принять и иную форму. Например, "Не понимаю, почему имеется особое правило для общих состояний для неотложенных задач, запущенных с помощью [[async|std::async]]. Это разумный вопрос. Я могу сказать, что Комитет по стандартизации хотел избежать проблем, связанных с неявным вызовом `detach`, но не хотел одобрять такую радикальную стратегию, как обязательное завершение программы (как сделано для подключаемых [[thread#std::thread|std::thread]]), так что в качестве компромисса был принят неявный вызов `join`. Это решение не без противоречий, и были серьезные предложения отказаться от этого поведения в С++14. В конце концов никакие изменения сделаны не были, так что поведение деструкторов фьючерсов в С++11 согласуется с таковым в С++14.

API для фьючерсов не предлагает способа определения, ссылается ли фьючерс на общее состояние, возникающее из вызова [[async|std::async]], так что невозможно узнать, будет ли заблокирован некоторый объект фьючерса в своем деструкторе для ожидания завершения асинхронно выполняющейся задачи. Это имеет некоторые интересные последствия.

```c++
// Этот контейнер может блокироваться в деструкторе , поскольку
// один или несколько содержащихся в нем фьючерсов могут
// ссылаться на общее состояние неотложенного задания,
// запущенного с помощью std::аsупс

std::vector<std::future<void>> futs ; // Cм. std::future<void>

class Widget {                        // Объекты Widget могут
	public:                           // блокироваться в их
									  // деструкторах
	private:
		std::shared_future<double>fut;
);
```

Конечно, если у вас есть способ узнать, что данный фьючерс не удовлетворяет условиям, приводящим к специальному поведению деструктора (например, в силу логики программы), вы можете быть уверены, что этот фьючерс не приведет к блокировке деструктора. Например, претендовать на особое поведение могут только общие состояния, получающиеся в результате вызовов [[async|std::async]], но есть и иные способы создания этих общих состояний. Один из них - использование [[packaged_task|std::packaged_task]]. Объект [[packaged_task|std::packaged_task]] подготавливает функцию (или иной вызываемый объект) к асинхронному выполнению, "заворачивая" ее таким образом, что ее результат помещается в общее состояние. Фьючерс, ссылающийся на это общее состояние, может быть получен с помощью функции `get_future` объекта [[packaged_task|std::packaged_task]].

```c++
int calcValue();                         // Выполняемая функция
std::packaged_task<int()>                // Заворачивание calcValue для
pt(calcValue);                           // асинхронного выполнения
auto fut = pt.get_future();              // Получение фьючерса для pt
```

В этой точке мы знаем, что фьючерс `fut` не ссылается на общее состояние, созданное вызовом [[async|std::async]], так что его деструктор будет вести себя нормально.

Будучи созданным, объект `pt` типа [[packaged_task|std::packaged_task]] может быть запущен в потоке. (Он может быть запущен и с помощью вызова [[async|std::async]], но если вы хотите выполнить задачу с использованием [[async|std::async]], то нет смысла создавать [[packaged_task|std::packaged_task]], поскольку [[async|std::async]] делает все, что делает [[packaged_task|std::packaged_task]] до того, как планировщик начинает выполнение задачи.)

Объекты [[packaged_task|std::packaged_task]] не копируются, так что когда `pt` передается в конструктор [[thread#std::thread|std::thread]], он должен быть приведен к `rvalue` (с помощью [[move|std::move]]).

```c++
std::thread t(std::move(pt));              // Вьmолнение pt потоком t
```

Этот пример дает некоторое представление о нормальном поведении деструкторов фьючерсов, но его легче увидеть, если собрать весь код вместе в одном блоке:

```c++
{                                               // Начало блока
	std::packaged_task<int()>
		pt(calcValue);
	
	auto fut = pt.get_future();
	std::thread t(std::move(pt));
}                                              // Конец блока
```

Наиболее интересный код скрывается за троеточием " ... ", следующим за созданием объекта `t` типа [[thread#std::thread|std::thread]] и предшествующим концу блока. Имеются три основные возможности.

> **С `t` ничего не происходит.** В этом случае `t` в конце области видимости будет неподключаемым. Это приведет к завершению программы.
> 
> **Для `t` вызывается функция-член `join`**. В этом случае фьючерсу `fut` не требуется блокировать деструктор, так как вызов `join` уже имеется в вызывающем коде.
> 
>  **Для `t` вызывается функция-член `detach`.** В этом случае фьючерсу `fut` не требуется вызывать `detach` в деструкторе, поскольку вызывающий код уже сделал это.

Другими словами, когда у вас есть фьючерс, соответствующий общему состоянию, получившемуся из-за применения [[packaged_task|std::packaged_task]], обычно не требуется принимать специальную стратегию деструкции, так как решение о прекращении выполнения программы, подключении или отключении потока принимается в коде, работающем с потоком [[thread#std::thread|std::thread]], в котором выполняется [[packaged_task|std::packaged_task]].

> Деструкторы фьючерсов обычно просто уничтожают данные-члены фьючерсов.
> 
> Последний фьючерс, ссылающийся на общее состояние неотложенной задачи, запущенной с помощью [[async|std::async]], блокируется до завершения этой задачи.

## Применяйте фьючерсы void для одноразовых сообщений о событиях

Иногда требуется, чтобы одна задача могла сообщить другой, выполняющейся асинхронно, о том, что произошло некоторое событие, поскольку вторая задача не может продолжать работу, пока это событие не произойдет. Например, пока не будет инициализирована структура данных, не будет завершено некоторое вычисление или не будет обнаружен сигнал от датчика. Какой в этом случае способ межпоточного сообщения является наилучшим?

Очевидный подход заключается в применении переменной условия. Если назвать задачу, которая обнаруживает условие, задачей обнаружения, а задачу, которая на него реагирует, задачей реакции, то выразить стратегию просто: задача реакции ожидает переменную условия, а поток задачи обнаружения выполняет ее уведомление при наступлении события. 

```c++
std::condition_variable cv;           // Переменная условия события
std::mutex m;                         // Мьютекс для использования с cv
```

код задачи обнаружения прост настолько, насколько это возможно:

```c++
...                                  // Обнаружение события

cv.notify_one();                    // Уведомление задачи реакции
```

Если требуется уведомить несколько задач реакции, можно заменить `notifу_one` на `notifу_аll`, но пока что будем считать, что у нас только одна задача реакции.

Код задачи реакции немного сложнее, поскольку перед вызовом `wait` для переменной условия он должен блокировать мьютекс с помощью объекта [[unique_lock|std::unique_lock]]. (Блокировка мьютекса перед ожиданием переменной условия типична для многопоточных библиотек. Необходимость блокировки мьютекса с помощью объекта [[unique_lock|std::unique_lock]] является частью API С++11.) Вот как выглядит концептуальный подход:

```c++
...                                       // Подготовка к реакции
{                                         // Открытие критического раздела
	std::unique_lock<std::mutex> lk(m);   // Блокировка мьютекса
	cv.wait(lk);                          // Ожидание уведомления;
    
    // неверно! Реакция на событие (m заблокирован) 
}                                         //Закрытие критического раздела;
// разблокирование m с помощью деструктора lk 
// Продолжение реакции (m разблокирован)
```

Первой проблемой при таком подходе является то, что часто называют кодом с душком (code smell): даже если команда работает, что-то выглядит не совсем верным. В нашем случае запах исходит от необходимости применения мьютексов. Мьютексы используются для управления доступом к совместно используемым данным, но вполне возможно, что для задач обнаружения и реакции такой посредник не требуется. Например, задача обнаружения может отвечать за инициализацию глобальной структуры данных, которая затем передается для использования задаче реакции. Если задача обнаружения никогда не обращается к структуре данных после ее инициализации и если задача реакции никогда не обращается к ней до того, как задача обнаружения укажет, что структура данных готова к использованию, эти две задачи оказываются не связанными логикой программы одна с другой. При этом нет никакой необходимости в мьютексе. Тот факт, что подход с использованием переменной условия требует применения мьютексов, оставляет тревожащий запашок подозрительного дизайна.

Даже если пропустить этот вопрос, все равно остаются две проблемы, которым, определенно, следует уделить внимание.

> **Если задача обнаружения уведомляет переменную условия до вызова `wait` задачей реакции, задача реакции "зависнет''.** Чтобы уведомление переменной условия активизировало другую задачу, эта другая задача должна находиться в состоянии ожидания переменной условия. Если вдруг задача обнаружения выполняет уведомление до того, как задача реакции выполняет `wait`, эта задача реакции пропустит уведомление и будет ждать его вечно.
> 
> **Вызов `wait` приводит к ложным пробуждениям.** В потоковых API (во многих языках программирования, не только в С++) не редкость ситуация, когда код, ожидающий переменную условия, может быть пробужден, даже если переменная условия не была уведомлена. Такое пробуждение называется ложным пробуждением (spurious wakeup). Корректный код обрабатывает такую ситуацию, проверяя, что ожидаемое условие в действительности выполнено, и это делается первым, немедленно после пробуждения. API переменных условия С++ делает это исключительно простым, поскольку допускает применение лямбда-выражений (или иных функциональных объектов), которые проверяют условие, переданное в `wait`. Таким образом, вызов `wait` в задаче реакции может быть записан следующим образом:
```c++
cv.wait(lk,
	[] {return Произошло ли событие; });
```

Применение этой возможности требует, чтобы задача реакции могла выяснить, истинно ли ожидаемое ею условие. Но в рассматриваемом нами сценарии ожидаемым условием является наступление события, за распознавание которого отвечает поток обнаружения. Поток реакции может быть не в состоянии определить, имело ли место ожидаемое событие. Вот почему он ожидает переменную условия!

Имеется много ситуаций, когда сообщение между задачами с помощью переменной условия вполне решает стоящую перед программистом проблему, но не похоже, что перед нами одна из них.

Многие разработчики используют совместно используемый булев флаг. Изначально этот флаг имеет значение `false`. Когда поток обнаружения распознает ожидаемое событие, он устанавливает этот флаг:

```c++
std::atomic<bool>flag(false);       // Совместно используемый флаг;
...                                 // Обнаружение события
flag = true;                        // Сообщение задаче обнаружения
```

Со своей стороны поток реакции просто опрашивает флаг. Когда он видит, что флаг установлен, он знает, что ожидаемое событие произошло:

```c++
...                                 // Подготовка к реакции
while(!flag);                       // Ожидание события
                                    // Реакция на событие
```

Этот подход не страдает ни одним из недостатков проекта на основе переменной условия. Нет необходимости в мьютексе, не проблема, если задача обнаружения устанавливает флаг до того, как задача реакции начинает опрос, и нет ничего подобного ложным пробуждениям. Хорошо, просто замечательно.

Куда менее замечательно выглядит стоимость опроса в задаче реакции. Во время ожидания флага задача, по сути, заблокирована, но продолжает выполняться. А раз так, она занимает аппаратный поток, который мог бы использоваться другой задачей, требует стоимости переключения контекста при каждом начале и завершении выделенных потоку временных промежутков и заставляет работать ядро, которое в противном случае могло бы быть отключено для экономии энергии. При настоящей блокировке задача не делает ничего из перечисленного. Это является преимуществом подхода на основе переменных условия, поскольку блокировка задачи при вызове `wait` является истинной.

Распространено сочетание подходов на основе переменных условия и флагов. Флаг указывает, произошло ли интересующее нас событие, но доступ к флагу синхронизирован мьютексом. Поскольку мьютекс предотвращает параллельный доступ к флагу, не требуется, чтобы флаг был объявлен как [[atomic|std::atomic]]; вполне достаточно простого `bool`. Задача обнаружения в этом случае может имеет следующий вид:

```c++
std::coпdition_variable cv;                 // Как и ранее
std::mutex m;
bооl flag(false);                           // Не std::atomic
...                                         // Обнаружение события
{
	std::lock_guard<std::mutex> g(m);       // Блокировка m
											// конструктором g
	flag = true;                            // Сообщаем задаче реакции
											// (часть 1)
}                                           // Снятие блокировки m
                                            // деструктором g
cv.notify_one();                            // Сообщаем задаче реакции
											// (часть 2)
```

А вот задача реакции:

```c++
...                                         // Подготовка к реакции
{                                           // Как и ранее
	std::unique_lock<std::mutex> lk(m);     // Как и ранее
	cv.wait(lk, [] { return flag; });       // Применение лямбда-
											// выражения во избежание
											// ложных пробуждений
											// Реакция на событие
											// (m заблокирован)
}
// Продолжение реакции
// (m разблокирован)
```

Этот подход позволяет избежать проблем, которые мы обсуждали. Он работает независимо от того, вызывает ли задача реакции `wait` до уведомления задачей обнаружения, он работает при наличии ложных пробуждений и не требует опроса флага. Тем не менее душок остается, потому что задача обнаружения взаимодействует с задачей реакции очень любопытным способом. Уведомляемая переменная условия говорит задаче реакции о том, что, вероятно, произошло ожидаемое событие, но задаче реакции необходимо проверить флаг, чтобы быть в этом уверенной. Установка флага говорит задаче реакции, что событие, определенно, произошло, но задача обнаружения по-прежнему обязана уведомить переменную условия о том, чтобы задача реакции активизировалась и проверила флаг. Этот подход работает, но не кажется очень чистым. 

Альтернативный вариант заключается в том, чтобы избежать переменных условия, мьютексов и флагов с помощью вызова `wait` задачей реакции для фьючерса, установленного задачей обнаружения. Это может показаться странной идеей. В конце концов [[Parallel computing#Помните о разном поведении деструкторов дескрипторов потоков|поясняется]], что фьючерс представляет принимающий конец канала связи от вызываемой функции к (обычно асинхронной) вызывающей функции, а между задачами обнаружения и реакции нет отношений "вызываемая-вызывающая': Однако [[Parallel computing#Помните о разном поведении деструкторов дескрипторов потоков|отмечается]], что канал связи, передающий конец которого представляет собой [[promise|std::promise]], а принимающий - фьючерс, может использоваться для большего, чем простой обмен информацией между вызываемой и вызывающей функциями. Такой канал связи может быть использован в любой ситуации, в которой необходима передача информации из одного места вашей программы в другое. В нашем случае мы воспользуемся им для передачи информации от задачи обнаружения задаче реакции, и информация, которую мы будем передавать - о том, что произошло интересующее нас событие.

Проект прост. Задача обнаружения имеет объект [[promise|std::promise]] (т.е. передающий конец канала связи), а задача реакции имеет соответствующий фьючерс. Когда задача обнаружения видит, что произошло ожидаемое событие, она устанавливает объект [[promise|std::promise]] (т.е. выполняет запись в канал связи). Тем временем задача реакции выполняет вызов `wait` своего фьючерса. Этот вызов `wait` блокирует задачу реакции до тех пор, пока не будет установлен объект [[promise|std::promise]].

И [[promise|std::promise]], и фьючерсы (т.е. [[future#std future|std::future]] и [[future#std shared_future|std::shared_future]]) являются шаблонами, требующими параметр типа. Этот параметр указывает тип данных, передаваемый по каналу связи. Однако в нашем случае никакие данные не передаются. Единственное, что представляет интерес для задачи реакции - что ее фьючерс установлен. Нам нужно указать для шаблонов [[promise|std::promise]] и фьючерса тип, показывающий, что по каналу связи не будут передаваться никакие данные. Таким типом является `void`. Задача обнаружения, таким образом, будет использовать `std::promise<void>`, а задача реакции - `std::future<void>` или `std::shared_ future<void>`. Задача обнаружения устанавливает свой объект `std::promise<void>`, когда происходит интересующее нас событие, а задача реакции ожидает с помощью вызова `wait` своего фьючерса. Даже несмотря на то, что задача реакции не получает никаких данных от задачи обнаружения, канал связи позволит задаче реакции узнать, что задача обнаружения "записала" vоid-данные с помощью вызова `set_value` своего объекта [[promise|std::promise]].

Так что для данного
```c++
std::promise<void> р;                    // Коммуникационный канал
```
код задачи обнаружения тривиален:
```c++
...                                     // Обнаружение события
р.set_value();                          // Сообщение задаче реакции
```
Код задачи реакции не менее прост:
```c++
...                                     // Подготовка к реакции
p.get_future().wait();                  // Ожидание фьючерса,
									    // соответствующего р
									    // Реакция на событие
```

Этот подход, как и использование флага, не требует мьютексов, работает независимо от того, устанавливает ли задача обнаружения свой объект [[promise|std::promise]] до того, как задача реакции вызывает `wait`, и невосприимчива к ложным пробуждениям. (Этой проблеме подвержены только переменные условия.) Подобно подходу на основе переменных условия задача реакции оказывается истинно заблокированной после вызова `wait`, так что во время ожидания не потребляет системные ресурсы. Идеально, нет?

Не совсем. Конечно, подход на основе фьючерсов обходит описанные неприятности, но ведь есть и другие. Например, в [[Parallel computing#Помните о разном поведении деструкторов дескрипторов потоков|разделе]] поясняется, что между [[promise|std::promise]] и фьючерсом находится общее состояние, а общие состояния обычно выделяются динамически. Поэтому следует предполагать, что данное решение приводит к расходам на динамическое выделение и освобождение памяти.

Вероятно, еще более важно то, что [[promise|std::promise]] может быть установлен только один раз. Канал связи между [[promise|std::promise]] и фьючерсом является одноразовым механизмом: он не может быть использован многократно. Это существенное отличие от применения переменных условия и флагов, которые могут использоваться для связи много раз. (Переменная условия может быть уведомлена неоднократно, а флаг может сбрасываться и устанавливаться вновь.)

Ограничение однократности не столь тяжкое, как можно подумать. Предположим, что вы хотите создать системный поток в приостановленном состоянии. То есть вы хотели бы заплатить все накладные расходы, связанные с созданием потока, заранее, с тем, чтобы как только вы будете готовы выполнить что-то в этом потоке, вы сможете делать это сразу, без задержки. Или, может быть, вы захотите создать поток в приостановленном состоянии с тем, чтобы можно было настроить его перед выполнением. Такая настройка может включать, например, установку приоритета. API параллельных вычислений С++ не предоставляет способ выполнить такие действия, но объекты [[thread#std::thread|std::thread]] предлагают функцию-член `native_handle`, результат которой призван предоставить доступ к API многопоточности используемой платформы (обычно потокам POSIX или Windows). Низкоуровневые API часто позволяют настраивать такие характеристики потоков, как приоритет или сродство.

В предположении, что требуется приостановить поток только один раз (после создания, но до запуска функции потока), можно воспользоваться vоid-фьючерсом. Вот как выглядит эта методика.

```c++
std::promise<void> р;
void react();                       // Функция потока реакции
void detect()                       // Функция потока обнаружения
{
	std::thread t([ ]               // Создание потока
		{
			p.get_future().wait();  // Приостановлен до
			react();                // установки фьючерса
		}) ;
									// Здесь t приостановлен	
									// до вызова react
	р.set_value();                  // Запуск t (и тем самым вызов react)
	...                             // Выполнение дополнительной работы
	t.join();                       // Делаем t неподключаемым
```

Поскольку важно, чтобы поток `t` стал неподключаемым на всех путях выполнения, ведущих из `detect`, применение RАII-класса наподобие [[Parallel computing#Делайте std thread неподключаемым на всех путях выполнения|класса ThreadRAll]] выглядит целесообразным. На ум приходит следующий код:

```c++
void detect()
{
	ThreadRAII tr(                   // RАII - объект 
		std::thread([ ]
				{
					p.get_future().wait();
					react();
				}),
		ThreadRAII::DtorAction::join // Рискованно! (См. ниже )
	);
	...                              // Поток в tr приостановлен
									 // Поток в tr разблокирован
}
```

Выглядит безопаснее, чем на самом деле. Проблема в том, что, если в первой области `"..."` (с комментарием "Поток в `tr` приостановлен") будет сгенерировано исключение, для `р` никогда не будет вызвана функция `set_value`. Это означает, что вызов `wait` в [[Lambda|лямбда выражении]] никогда не завершится. А это, в свою очередь, означает, что поток, выполняющий [[Lambda|лямбда-выражение]], никогда не завершается, а это представляет собой проблему, поскольку RАII-объект `tr` сконфигурирован для выполнения `join` для этого потока в деструкторе. Другими словами, если в первой области кода `"..."` будет сгенерировано исключение, эта функция "зависнет", поскольку деструктор `tr` никогда не завершится.

Здесь я хотел бы показать, как исходный код (т.е. без применения `ThreadRAII`) может быть расширен для приостановки и последующего продолжения не одной задачи реакции, а нескольких. Это простое обобщение, поскольку ключом является применение [[future#std shared_future|std::shared_future]] вместо [[future#std future|std::future]] в коде `react`. Как вы уже знаете, функция-член `share` объекта [[future#std future|std::future]] передает владение его общим состоянием объекту [[future#std shared_future|std::shared_future]], созданному `share`, а после этого код пишется почти сам по себе. Единственной тонкостью является то, что каждый поток реакции требует собственную копию [[future#std shared_future|std::shared_future]], которая ссылается на общее состояние, так что объект [[future#std shared_future|std::shared_future]], полученный от `share`, захватывается по значению [[Lambda|лямбда выражением]], запускаемым в потоке реакции:

```c++
std::promise<void> р;                   // Как и ранее
void detect()                           // Теперь для нескольких
{                                       // задач реакции
	auto sf = p.get_future().share();   // Тип sf -
										// std::shared future<void>
	std::vector<std::thread> vt;        // Контейнер для потоков
										// реакции
	for (int i = О; i < threadsToRun; ++i) {
										// Ожидание локальной копии sf
										// см. emplace_back
		vt.emplace_back([sf] {sf.wait(); 
								react(); });	
	}
										// detect "зависает", если
										// десь генерируется
										// исключение!
	p.set_value();                      // Продолжение всех потоков
	...
	for(auto& t : vt){                  // Все потоки делаются
		t.join();                       // неподключаемыми;
	}
}
```

Примечателен тот факт, что дизайн с помощью фьючерсов позволяет добиться описанного эффекта, так что поэтому следует рассматривать возможность его применения там, где требуется одноразовое сообщение о событии.

> Для простого сообщения о событии дизайн с применением переменных условия требует избыточных мьютексов, накладывает ограничения на относительное выполнение задач обнаружения и реакции и требует от задачи реакции проверки того, что событие в действительности имело место.
>
> Дизайн с использованием флага устраняет эти проблемы, но использует опрос, а не блокировку.
> 
> Переменные условия и флаги могут быть использованы совместно, но получающийся механизм сообщений оказывается несколько неестественным.
> 
> Применение объектов [[promise|std::promise]] и фьючерсов решает указанные проблемы, но этот подход использует динамическую память для общих состояний и ограничен одноразовым сообщением.

## Используйте std::аtomic для параллельности, volatile - для особой памяти

Бедный квалификатор [[volatile|volatile]]! Такой неверно понимаемый . . . Его даже не должно быть в этой главе, потому что он не имеет ничего общего с параллельным программированием. Но в других языках (например, в Java и С#) он полезен для такого программирования, и даже в С++ некоторые компиляторы перенасыщены [[volatile|volatile]] с семантикой, делающей его применимым для параллельного программирования (но только при компиляции этими конкретными компиляторами). Таким образом, имеет смысл обсудить [[volatile|volatile]] в главе, посвященной параллельным вычислениям, хотя бы для того, чтобы развеять окружающую его путаницу.

Возможность С++, которую программисты периодически путают с [[volatile|volаtilе]] и которая, безусловно, относится к данной главе, - это шаблон [[atomic|std::atomic]]. Инстанцирования этого шаблона (например, `std::atomic<int>`, `std::atomic<bool>`, `std::atomic<Widget *>` и т.п.) предоставляют операции, которые другими потоками будут гарантированно восприниматься как атомарные. После создания объекта [[atomic|std::atomic]] операции над ним ведут себя так, как будто они выполняются внутри критического раздела, защищенного мьютексом, но эти операции обычно реализуются с помощью специальных машинных команд, которые значительно эффективнее применения мьютексов.

Рассмотрим код с применением [[atomic|std::atomic]]:

```c++
std::atomic<int> ai(0);           // Инициализация ai значением О
ai = 10;                          // Атомарное присваивание ai значения 10
std::cout << ai;                  // Атомарное чтение значения ai
++ai;                             // Атомарный инкремент ai до 11
--ai;                             // Атомарный декремент ai до 10
```

В процессе выполнения данных инструкций другие потоки, читающие `ai`, могут увидеть только значения 0, 10 и 11. Никакие другие значения невозможны (конечно, в предположении, что это единственный поток, модифицирующий `ai`).

Следует отметить два аспекта этого примера. Во-первых, в инструкции
`"std::cout << ai;"` тот факт, что `ai` представляет собой [[atomic|std::atomic]], гарантирует только то, что атомарным является чтение `ai`. Нет никакой гарантии атомарности всей инструкции. Между моментом чтения значения `ai` и вызовом оператора `operator <<` для записи в поток стандартного вывода другой поток может изменить значение `ai`. Это не влияет на поведение инструкции, поскольку `operator<<` для `int` использует передачу выводимого параметра типа `int` по значению (таким образом, выведенное значение будет тем, которое прочитано из `ai`), но важно понимать, что во всей этой инструкции атомарным является только чтение значения `ai`.

Второй важный аспект этого примера заключается в поведении двух последних инструкций - инкремента и декремента `ai`. Каждая из этих операций является операцией чтения-изменения-записи (read-modify-write - RMW), выполняемой атомарно. Это одна из приятнейших характеристик типов [[atomic|std::atomic]]: если объект [[atomic|std::atomic]] создан, все его функции-члены, включая RМW-операции, будут гарантированно рассматриваться другими потоками как атомарные.

В противоположность этому такой же код, но использующий квалификатор [[volatile|volatile]], в многопоточном контексте не гарантирует почти ничего:

```c++
volatile vi(0);                   // Инициализация ai значением О
vi = 10;                          // Атомарное присваивание ai значения 10
std::cout << vi;                  // Атомарное чтение значения ai
++vi;                             // Атомарный инкремент ai до 11
--vi;                             // Атомарный декремент ai до 10
```

Во время выполнения этого кода, если друтой поток читает значение `vi`, он может прочесть что утодно, например `- 12, 68, 4090727`, - любое значение! Такой код обладает неопределенным поведением, потому что эти инструкции изменяют `vi`, и если другие потоки читают `vi` в тот же момент времени, то эти одновременные чтения и записи памяти не защищены ни [[atomic|std::atomiс]], ни с помощью мьютексов, а это и есть определение гонки данных.

В качестве конкретного примера того, как отличаются поведения [[atomic|std::аtomic]] и [[volatile|volatile]] в многопоточной программе, рассмотрим простые счетчики каждого вида, увеличиваемые несколькими потоками. Инициализируем каждый из них значением 0:

```c++
std::atomic<int> ас(О);             // "счетчик atomic"
volatile int vc(O);                 // "счетчик volatile"
```

Затем увеличим каждый счетчик по разу в двух одновременно работающих потоках:

```
/* ----- Поток 1 ----- * / / * ----- Поток 2 ----- */
		   ++ас;                      ++ас;
	       ++vc;                      ++vc;
```

По завершении обоих потоков значение `ас` (т.е. значение [[atomic|std::atomic]]) должно быть равно `2`, поскольку каждый инкремент осуществляется как атомарная, неделимая операция. Значение `vc`, с другой стороны, не обязано быть равным `2`, поскольку его инкремент может не быть атомарным. Каждый инкремент состоит из чтения значения `vc`, увеличения прочитанного значения и записи результата обратно в `vc`. Но для объектов [[volatile|volatile]] не гарантируется атомарность всех трех операций, так что части двух инкрементов `vc` могут чередоваться следующим образом.

1. Поток 1 считывает значение `vc`, равное 0.
2. Поток 2 считывает значение `vc`, все еще равное 0.
3. Поток 1 увеличивает 0 до 1 и записывает это значение в `vc`.
4. Поток 1 увеличивает 0 до 1 и записывает это значение в `vc`.

Таким образом, окончательное значение `vc` оказывается равным 1, несмотря на два инкремента.

Это не единственный возможный результат. В общем случае окончательное значение `vc` непредсказуемо, поскольку переменная `vc` включена в гонку данных, а стандарт однозначно утверждает, что гонка данных ведет к неопределенному поведению; это означает, что компиляторы могут генерировать код, выполняющий буквально все что угодно. Конечно, компиляторы не используют эту свободу для чего-то вредоносного. Но они могут выполнить оптимизации, вполне корректные при отсутствии гонки данных, и эти оптимизации приведут к неопределенному и непредсказуемому поведению при наличии гонки данных.

RМW-операции - не единственная ситуация, в которой применение [[atomic|std::atomic]] ведет к успешным параллельным вычислениям, а [[volatile|volatile]] - к неудачным. Предположим, что одна задача вычисляет важное значение, требуемое для второй задачи. Когда первая задача вычисляет значение, она должна сообщить об этом второй задаче. В [[Parallel computing#Применяйте фьючерсы void для одноразовых сообщений о событиях|разделе]] поясняется, что одним из способов, которым один поток может сообщить о доступности требуемого значения другому потоку, является применение `std::atomic<bool>`. Код в задаче, выполняющей вычисление значения, имеет следующий вид:

```c++
std::atomic<bool> valAvailable(false);
auto imptValue = computeImportantValue();   // Вычисление значения
valAvailable = true;                        // Сообщение об этом
                                            // другому потоку
```

Как люди, читая этот код, мы знаем, что критично важно, чтобы присваивание `imptValue` имело место до присваивания `valAvailable`, но все компиляторы видят здесь просто пару присваиваний независимым переменным. В общем случае компиляторы имеют право переупорядочить такие независимые присваивания. Иначе говоря, последовательность присваиваний (где `а`, `b`, `x` и `у` соответствуют независимым переменным)

```c++
а = Ь;
х = у;
```

компиляторы могут переупорядочить следующим образом:

```c++
х = у;
а = Ь;
```

Даже если такое переупорядочение выполнено не будет, это может сделать аппаратное обеспечение (или сделать его видимым таковым для других ядер. если таковые имеются в наличии), поскольку иногда это может сделать код более быстрым.

Однако применение [[atomic|std::atomic]] накладывает ограничения на разрешенные переупорядочения кода, и одно такое ограничение заключается в том, что никакой код, предшествующий в исходном тексте записи переменной [[atomic|std::аtomiс]], не может иметь место (или выглядеть таковым для других ядер) после нее. Это означает, что в нашем коде

```c++
auto imptValue = computeImportantValue(); // Вычисление значения
valAvailable = true;                      // Сообщение об этом
										  // другому потоку
```

компиляторы должны не только сохранять порядок присваиваний `imptValue` и `valAvailable`, но и генерировать код, который гарантирует, что так же поведет себя и аппаратное обеспечение. В результате объявление `valAvailable` как [[atomic|std::atomic]] гарантирует выполнение критичного требования к упорядоченности - что значение `imptValue` должно быть видимо всеми потоками как измененное не позже, чем значение `valAvailable`.

Объявление `valAvailable` как [[volatile|volatile]] не накладывает такое ограничение на переупорядочение кода:

```c++
volatile bool valAvailable(false);
auto imptValue = computeImportantValue();
valAvailable = true;        // Другие потоки могут увидеть это
							// присваивание до присваивания imptValue!
```

Здесь компиляторы могут изменить порядок присваиваний переменным `imptValue` и `valAvailable`, но даже если они этого не сделают, они могут не сгенерировать машинный код, который предотвратит возможность аппаратному обеспечению сделать так, что другие ядра увидят изменение `valAvailable` до изменения `imptValue`.

Эти две проблемы - отсутствие гарантии атомарности операции и недостаточные ограничения на переупорядочение кода - поясняют, почему [[volatile|volatile]] бесполезен для параллельного программирования, но не поясняют, для чего же этот квалификатор полезен. В двух словах - чтобы сообщать компиляторам, что они имеют дело с памятью, которая не ведет себя нормально.

"Нормальная", "обычная" память обладает тем свойством, что если вы записываете в нее значение, то оно остается неизменным, пока не будет перезаписано. Так что если у меня есть обычный `int`
```c++
iпt х ;
```
и компилятор видит последовательность операций
```c++
auto у = х;                       // Чтение х
y = x;                            // Чтение х еще раз
```
то он может оптимизировать генерируемый код, убрав присваивание переменной `у`, поскольку оно является излишним из-за инициализации `у`.

Обычная память обладает также тем свойством, что если вы запишете значение в ячейку памяти, никогда не будете его читать, а потом запишете туда же что-то еще, то первую запись можно не выполнять, потому что записанное ею значение никогда не используется. С учетом этого в коде
```c++
х = 10;                          // Запись х
х = 20;                          // Запись х еще раэ
```
компиляторы могут убрать первую инструкцию. Это означает, что если у нас имеется код
```c++
auto у = х ;                     // Чтение х
y = х;                           // Чтение х еще раэ
х = 10;                          // Запись х
х = 20;                          // Запись х еще раэ
```
то компиляторы могут рассматривать его, как если бы он имел следующий вид:
```c++
auto у = х;                      // Чтение х
х = 20;                          // Запись х
```

Чтобы вас не мучило любопытство, кто в состоянии написать такой код с избыточными чтениями и лишними записями (технически известными как избыточные загрузки (redundant loads) и бессмысленные сохранения (dead stores)), отвечу: нет, люди не пишут непосредственно такой код, по крайней мере я очень на это надеюсь. Однако после того как компиляторы получают разумно выглядящий код и выполняют инстанцирования шаблонов, встраивание кода и различные виды переупорядочивающих оптимизаций, в результате не так уже редко получаются и избыточные загрузки, и бессмысленные сохранения, от которых компиляторы могут избавиться.

Такие оптимизации корректны, только если память ведет себя нормально. "Особая" память так себя не ведет. Наиболее распространенным видом особой памяти, вероятно, является память, используемая для отображенного на память ввода-вывода. Вместо чтения и записи обычной памяти, местоположения в такой особой памяти в действительности сообщаются с периферийными устройствами, например внешними датчиками или мониторами, принтерами, сетевыми портами и т.п. Давайте с учетом этого снова рассмотрим код с, казалось бы, избыточными чтениями:

```c++
auto у = х;                      // Чтение х
y = x;                           // Чтение х еще раз
```

Если `х` соответствует, скажем, значению, которое передает датчик температуры, то второе чтение `х` избыточным не является, поскольку температура между первым и вторым чтениями может измениться.

Похожа ситуация с записями, кажущимися излишними. Например, если в коде
```c++
х = 10;                           // Запись х
x = 20;                           // Запись х еще раз
```

переменная `х` соответствует управляющему порту радиопередатчика, может оказаться, что этот код выполняет некоторые команды с радиопередатчиком, и значение `10` соответствует команде, отличной от имеющей код `20`. Оптимизация, убирающая первое присваивание, могла бы изменить последовательность команд, отправляемых радиопередатчику.

Квалификатор [[volatile|volatile]] представляет собой способ сообщить компиляторам, что мы имеем дело с такой особой памятью. Для компилятора это означает "не выполняй никаких оптимизаций над операциями с этой памятью": Так что если переменная `х` соответствует особой памяти, она должна быть объявлена как [[volatile|volatile]]:

```c++
volatile iпt х;
```

Рассмотрим влияние этого квалификатора на последовательность нашего исходного кода:

```c++
auto у = х;                  // Чтение х
y = х;                       // Чтение х еще раз (не может быть устранено)
х = 10;                      // Запись х (не может быть устранена)
x = 20;                      // Запись х еще раз
```

Это именно то, чего мы хотим, когда `х` представляет собой отображение в память (или отображается в ячейке памяти, совместно используемой разными процессами и т.п.).

Тот факт, что кажущиеся избыточными загрузки и бессмысленные сохранения должны оставаться на месте при работе с особой памятью, объясняет, кстати, почему для такого рода работы не подходят объекты [[atomic|std::atomic]]. Компиляторам разрешается устранять такие избыточные операции у [[atomic|std::atomic]]. Код написан не в точности так же, как и для [[volatile|volatile]], но если мы на минуту отвлечемся от этого и сосредоточимся на том, что компиляторам разрешается делать, то можно сказать, что концептуально компиляторы могут, получив код

```c++
std::atomic<int> х;
auto у = х;                   // Концептуально читает х (см. ниже)
у = х;                        // Концептуально читает х еще раз (см. ниже)
х = 10;                       // Записывает х
х = 20;                       // Записывает х еще раз
```

оптимизировать его до

```c++
auto у = х;                   // Концептуально читает х (см. ниже)
х = 20;                       // Записывает х
```

Очевидно, что это неприемлемое поведение при работе с особой памятью.

Но если `х` имеет тип [[atomic|std::atomic]], ни одна из этих инструкций компилироваться не будет:

```c++
auto у = х;                          // Ошибка !
y = х;                               // Ошибка !
```

Дело в том, что [[Предпочитайте удаленные функции закрытым неопределенным|копирующие операции]] в [[atomic|std::atomic]] удалены. И не зря. Рассмотрим, что произошло бы, если бы инициализация `у` значением `х` компилировалась. Поскольку `х` имеет тип [[atomic|std::atomic]], тип у был бы также выведен как [[atomic|std::atomic]]. Ранее я отмечал, что одна из лучших возможностей [[atomic|std::atomic]] заключается в атомарности всех их операций, но чтобы копирующее конструирование `у` из `х` было атомарным, компиляторы должны генерировать код для чтения `х` и записи `у` как единую атомарную операцию. В общем случае аппаратное обеспечение не в состоянии это сделать, так что копирующее конструирование типами [[atomic|std::atomic]] не поддерживается. Копирующее присваивание удалено по той же причине, а потому присваивание `х` переменной `у` также не компилируется. (Перемещающие операции не объявлены в [[atomic|std::atomic]] явно, так что в соответствии с [[Генерация специальных функций-членов|правилами генерации специальных функций]], [[atomic|std::atomic]] не предоставляет ни перемещающего конструирования, ни перемещающего присваивания.)

Можно получить значение `х` в переменную `у`, но это требует использования функций-членов `load` и `store` типа [[atomic|std::atomic]]. Функция-член `load` атомарно считывает значение [[atomic|std::atomic]], а функция-член `store` атомарно его записывает. Для инициализации `у` значением `х`, после которой выполняется размещение значения `х` в `у`, код должен иметь следующий вид:

```c++
std::atomic<int> y(x.load());            // Чтение х
y.store(x.load());                       // Чтение х еще раз
```

Этот код компилируется, но тот факт, что чтение `х` (с помощью `х.load()`) является отдельным от инициализации или сохранения значения `у` вызовом функции, делает очевидным то, что нет никаких оснований ожидать, что целая инструкция будет выполняться как единая атомарная операция.

Компиляторы могут "оптимизировать" приведенный код, сохраняя значение `х` в регистре вместо двойного его чтения:

```c++
register = x.load();             // Чтение х в регистр

std::atomic<int> y(register);    // Инициализация у
                                 // значением регистра
у.store(register);               // Сохранение значения
								 // регистра в у
```

В результате, как можно видеть, чтение из `х` выполняется только один раз, и этой разновидности оптимизации следует избегать при работе с особой памятью. (Эта оптимизация не разрешена при работе с переменными [[volatile|volatile]].)

Таким образом, ситуация должна быть очевидна.

> [[atomic|std::atomic]] применяется в параллельном программировании, но не для доступа к особой памяти.
> 
> [[volatile|volatile]] применяется для доступа к особой памяти, но не в параллельном программировании.

Поскольку [[atomic|std::atomic]] и [[volatile|volatile]] служат разным целям, они могут использоваться совместно:

```c++
volatile std::atomic<int> vai;      // Операции над vai атомарны
									// и не могут быть удалены
									// при оптимизации
```

Этот может оказаться полезным, когда `vai` соответствует ячейке отображаемого на память ввода-вывода, обращение к которой выполняется несколькими потоками.

Последнее примечание: некоторые разработчики предпочитают использовать функции-члены `load` и `store` типа [[atomic|std::atomic]] даже там, где это не требуется, поскольку это четко указывает в исходном тексте на то, что данные переменные не являются "обычными": Подчеркивание этого факта не является необоснованным. Доступ к [[atomic|std::atomic]] обычно гораздо медленнее, чем к переменным, не являющимся [[atomic|std::atomic]], и мы уже видели, что использование [[atomic|std::atomic]] предотвращает определенное переупорядочение кода, которое иначе было бы разрешено. Вызовы загрузок и сохранений [[atomic|std::atomic]] могут тем самым помочь в идентификации потенциальных узких мест масштабируемости. С точки зрения корректности отсутствие вызова `store` у переменной, предназначенной для передачи информации в другие потоки (например, флаг, указывающий доступность данных), может означать, что эта переменная не объявлена как [[atomic|std::atomic]], хотя должна быть таковой.

Однако в большей степени это вопрос стиля, и как таковой он не имеет отношения к выбору между [[atomic|std::atomic]] и [[volatile|volatile]].

> [[atomic|std::atomic]] применяется для обращения нескольких потоков к данным без использования мьютексов. Это инструмент параллельного программирования.
> 
> [[volatile|volatile]] применяется для памяти, чтения и записи которой не должны удаляться при оптимизации. Это инструмент для работы с особой памятью.




















